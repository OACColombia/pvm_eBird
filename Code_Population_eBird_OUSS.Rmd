---
title: "Risk-based viable population monitoring from community science data"
author: "Authors...(removed for peer review in Methods in Ecology and Evolution)"
date: "`r format(Sys.time(), '%B %Y')`"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Read me (begin here)

This file is the code used for a paper *to be submitted* to ***Methods in Ecology and Evolution***. Thus, we hope this program is applied for different users in an intuitive and practical way. 

Our aim is to provide a rapid assessment of short trends of local persistence probability ($\phi$) by incorporating population dynamics robust models (Dennis & Ponciano, 2014; Humbert *et al.*, 2009) withing the risk-based population viability monitoring framework (Staples *et al.*, 2005) using time series data from eBird. Thus, we begin by organizing eBird data records in a time series of weekly high counts estimates, filtering by best practices recommended elsewhere (Backstrom *et al*., 2024; Johnston e*t al.*, 2021; Kelling *et al.*, 2019; Strimas-Mackey *et al.*, 2023).  

We focused on the community science platform [eBird](https://ebird.org/home), with an example that have long-term monitoring efforts to understand population dynamics, the Snail Kite in Florida [*Rostrhamus sociabilis*](https://birdsoftheworld.org/bow/species/snakit/cur/introduction) (see *Prepare the data from eBird and time-series vectors*). Specifically, we concentrate our analysis on the population of Snail kites in the Payne's Prairie wetland, in Alachua County, northcentral Florida. We compared our approach using eBird data with long-term standardized monitoring efforts of the species in the same locality, fitting continuous version of the discrete-time equal sampling Gompertz State-Space model: 

1. The Ornstein-Uhlenbeck State-Space model (OUSS)
2. The Exponential Growth (or decay) State Space model (EGSS)

Applying a risk-based viable population monitoring framework to these data sets (see Staples *et al.*, 2005).


The statistical properties for our proceedings are based on Dennis *et al.* (1991), Dennis *et al.* (2006), Dennis & Ponciano (2014), Humbert *et al.* (2009), and reference within them. We adjusted the functions and code published as supplementary information in Humbert *et al.* (2009) and Dennis & Ponciano (2014) (`rouss`). 

# Packages

The code will require some packages. If you do not have these packages, please install them before proceeding. We encourage to study each package to be familiar with their functions.

```{r message=FALSE}
#R functions and datasets to support "Modern Applied Statistics with S", a book from W.N. Venables and B.D. Ripley
  library(MASS);
#Lognormal distribution
  library(kde1d)
#To conduct eBird data filtering and manipulation (see Strimas et al. 2018 and 2023)
  library(auk); 
#To data management and visualization - sevent packages in one
  library(tidyverse)
#To conduct Spatiotemporal Subsampling
  library(dggridR)
#load maps
  library(maps); 
#composite figure
  library(gridExtra); 

#package under development
#library(devtools);
#install_github("OACColombia/rouss");
#library(rouss);
```

# Functions

The following functions should be available to the analysis of time-series of observation count data from eBird. Although we deploy some of their statistical properties and equations to justify their use, most information is also available in the references mentioned in the *Read me (begin here)* section.

## Convert time observation to hours since midgnight in eBird data

```{r}
time_to_decimal <- function(x) {
  x <- hms(x, quiet = TRUE)
  hour(x) + minute(x) / 60 + second(x) / 3600
}
```

## EGSS-MLE

The first example in any population ecology class is the exponential growth population model. This model assumes density independence and it could be of an stochastic form. The model is defined as the stochastic differential equation (from the Itô log-transformation):

$$
\begin{array}{ccc}
dX(t) &=& (r-\frac{1}{2} \beta^2)dt+\beta dW(t) \\
&=& \theta\ dt+\beta dW(t)
\end{array}
$$
where $X(t) = \ln{N(t)}$, the log population abundance at time $t$, $\theta = r-\frac{1}{2}\beta^2$ is a constant of population growth rate ($\ln{\lambda}-(\frac{\sigma^2}{2})$ as in the ***Eq13*** in Dennis *et al.*, 1991; named $\mu = \ln{\lambda}$ in Humbert *et al.*, 2009), and $dW(t)$ is a random perturbation representing the environmental stochasticity or process noise (the Itô transformation of $E_t$ from GSS in Dennis *et al.*, 2006), with mean $0$ and variance $\sigma^2dt$. Estimated or observed abundance $Y(t_i)$ of the logarithmic population abundance $X(t_i)$ has an observation error $F_i$ with mean $0$ and variance $\tau^2$:

$$
\begin{array}{ccc}
Y(t_i) &=& X(t_i) + F_i \\
F_i &\sim& \text{Normal}(0,\tau^2)
\end{array}
$$


EGSS have a multivariate normal log-likelihood function given by (***EqA17*** in Humbert *et al.*, 2009):

$$
\ln{L}(\theta, \sigma^2, \tau^2, x_0) = -\frac{(q+1)}{2}\ln(2\pi)-\frac{1}{2}\ln(|\mathbf{V}|)-\frac{1}{2}(\mathbf{y}-\mathbf{m})'\mathbf{V}^{-1}(\mathbf{y}-\mathbf{m})
$$
where $q+1$ represent the length of the time-series, $\mathbf{V}$ is the variance-covariance matrix, $\mathbf{y}$ is the vector of log-abundance observations ($y_0$, $y_1$, ..., $y_q$), and $\mathbf{m}$ is the vector of means of change from $x_0$ to $x_0+\theta t_1$, to $x_0+\theta t_2$, ... $x_0+\theta t_q$.

The four unknown parameters are: $\theta$ (trend parameter, or expected change under density-independence), $\sigma^2$ (variation of the process noise), $\tau^2$ (variation of the observation noise), and $x_0$ (the unknown initial population).
 
For the numerical optimization of this multivariate normal log-likelihood, we will need three arguments:

1.  A vector of time-series log-observed abundances `yt` ($\mathbf{y}$)
2.  A vector of observation times `tt`
3.  A vector of initial parameters `fguess_egss` (a first guess for the **four** parameters in EGSS), that could be roughly computing by provide the vector of log-abundance observations `yt` ($\mathbf{y}$) and the vector of observation times `tt` with function `guess_egss()`

```{r}
guess_egss <- function(yt,tt){
	
  # Time-vector starting in 0.
    t.i       <- tt-tt[1];   
  # Number of time-series transitions
    q         <- length(yt)-1;   
  # length of time-series
    qp1       <- q+1;
  # time intervals (named as S.t in H_O and sometimes in DP_E)
    t.s       <- t.i[2:qp1]-t.i[1:q];  
    
  #The Exponential Growth Observation Error (EGOE in H_O) initial values  
  
  # mean of the observations as assumed to arise from stationary distribution
    Ybar      <- mean(yt);
	# mean of the time series
    Tbar      <- mean(t.i)
	# trend parameter for EGOE (theta = ln(lambda) in H_O)
    theta.egoe    <- sum((t.i-Tbar)*(yt-Ybar))/sum((t.i-Tbar)*(t.i-Tbar));
	# Initial population  of EGOE
    x0.egoe   <- Ybar-theta.egoe*Tbar
	# sigma square for EGOE is 0 (assume no ecological process variation)
    ssq.egoe  <- 0
	# estimate of initial population observed under EGOE
    Yhat.egoe <- x0.egoe+theta.egoe*t.i;
	# initial value for tau^2
    tsq.egoe  <- sum((yt-Yhat.egoe)*(yt-Yhat.egoe))/(q-1);
	 
  #The Exponential Growth Process Noise (EGPN in H_O) initial values  
	# Square root of time intervals (time trend?)
    Ttr       <- sqrt(t.s);
	# Observed trend?   
    Ytr       <- (yt[2:qp1]- yt[1:q])/Ttr;
	# trend parameter for EGPN (mu = ln(lambda) in H_O) 
    theta.egpn    <- sum(Ttr*Ytr)/sum(Ttr*Ttr);
	# Trend of observed estimated  
    Ytrhat    <- theta.egpn*Ttr;
	# initial value for sigma^2  
    ssq.egpn  <- sum((Ytr-Ytrhat)*(Ytr-Ytrhat))/(q-1);
	# tau square for EGPN is 0 (assume no observation variation)
    tsq.egpn  <- 0;
	# Initial population  of EGPN is the first observation
    x0.egpn   <- yt[1];

	#four parameters needed in EGSS and OUSS.NoSt 	  
    theta0    <- (theta.egoe+theta.egpn)/2;
    ssq0      <- ssq.egpn/2;
    tsq0      <- tsq.egoe/2;
    x0.out    <- (x0.egoe+x0.egpn)/2;
	
  return(c(theta0, ssq0, tsq0, x0.out))
  }
```

The numerical optimization for the MLEs in `R` is:

```{r}
negloglike_egss_mle <- function(yt,tt,fguess_egss){
	
    theta         <- fguess_egss[1];
	  sigmasq       <- exp(fguess_egss[2]);
	  tausq         <- exp(fguess_egss[3]);
	  xo            <- fguess_egss[4];
	  q             <- length(yt) - 1;
	  qp1           <- q+1;
	  yt            <- matrix(yt,nrow=qp1,ncol=1); # makes data a matrix object
	  vx            <- matrix(0,qp1,qp1);
	for(i in 1:q){ 
		vx[((i+1):qp1),((i+1):qp1)] <- matrix(1,(qp1-i),(qp1-i))*tt1[(i+1)];
	}
	  Sigma.mat     <- sigmasq*vx;
	  Itausq        <- matrix(rep(0,(qp1*qp1)), nrow=qp1, ncol=qp1);
	  diag(Itausq)  <- rep(tausq,qp1);
	  V             <- Sigma.mat + Itausq;
	  theta.vec     <- matrix((xo+theta*tt1), nrow=qp1,ncol=1);
	
	return((qp1/2)*log(2*pi) + 0.5*log(det(V)) + 0.5*(t(yt-theta.vec)%*%ginv(V)%*%(yt-theta.vec)))	
}
```

Now we can compute the MLE of the EGSS model with the function `egss_mle()`:

```{r}
egss_mle <- function(yt,tt,fguess_egss){
	
  # Time-vector starting in 0.
    t.i         <- tt-tt[1];
  # Number of time-series transitions
    q           <- length(t.i)-1;
  # length of time-series
    qp1         <- q+1;
  # initial guesses (r as estimated, sigmasq and tausq at log scale (?), x0 as estimated)
    guess.optim <- c(fguess_egss[1], 
                     log(fguess_egss[2:3]), 
                     fguess_egss[4]) 
  # numerical optimization
    optim.out   <- optim(par=guess.optim, 
	                   fn=negloglike_egss_mle, 
	                   method="Nelder-Mead", 
	                   yt=yt, 
	                   tt=t.i)
  #Extract MLEs and AIC
    mles        <- c(optim.out$par[1], 
                     exp(optim.out$par[2:3]), 
                     optim.out$par[4])
    lnL.hat     <- - optim.out$value[1]
    AIC         <- -2*lnL.hat + 2*4 #where 4 = length(MLEs)... 
	
    out         <- list(mles=mles, 
                        lnL.hat = lnL.hat, 
                        AIC=AIC)
	return(out)
}
```

## EGSS-REMLE (Some problems)

The numerical optimization for the REMLEs in `R` is:

```{r}
negloglike_egss_remle <- function(yt,tt,fguess_egss){
	
    #theta         <- fguess_egss[1];
	  sigmasq       <- exp(fguess_egss[2]);
	  tausq         <- exp(fguess_egss[3]);
	  #xo            <- fguess_egss[4];
	  q             <- length(yt) - 1;
	  qp1           <- q+1;
	  vx            <- matrix(0,qp1,qp1);
	  for(i in 1:q){ 
		  vx[(i+1):qp1,(i+1):qp1] <- matrix(1,(qp1-i),(qp1-i))*tt[i+1];
	  }
	  Sigma.mat     <- sigmasq*vx;
	  Itausq        <- matrix(rep(0,(qp1*qp1)), nrow=qp1, ncol=qp1);
	  diag(Itausq)  <- rep(tausq,qp1);
	  V             <- Sigma.mat + Itausq;
	  ss=tt[2:qp1]-tt[1:q]; 
	  D1mat=cbind(-diag(1/ss),matrix(0,q,1))+cbind(matrix(0,q,1),diag(1/ss));
	  D2mat=cbind(-diag(1,q-1),matrix(0,q-1,1)) + cbind(matrix(0,q-1,1),diag(1,q-1));
	  Phi.mat=D2mat%*%D1mat%*%V%*%t(D1mat)%*%t(D2mat); 
	  wt=(yt[2:qp1]-yt[1:q])/ss; 
	  ut=wt[2:q]-wt[1:q-1]; 
	  ofn=((qp1)/2)*log(2*pi)+(0.5*log(det(Phi.mat))) + (0.5*(ut%*%ginv(Phi.mat)%*%ut)); 
	  
	  return(ofn)
}
```

Now we can compute the MLE of the EGSS model with the function `egss_remle()`:

```{r}
egss_remle <- function(yt,tt,fguess_egss){
	
  # Time-vector starting in 0.
    t.i         <- tt-tt[1];
  # Number of time-series transitions
    q           <- length(t.i)-1;
  # length of time-series
    qp1         <- q+1;
  # initial guesses (r as estimated, sigmasq and tausq at log scale (?), x0 as estimated)
    guess.optim <- c(log(fguess_egss[2:3])) 
  # numerical optimization
    optim.out   <- optim(par=guess.optim, 
	                   fn=negloglike_egss_remle, 
	                   method="Nelder-Mead", 
	                   yt=yt, 
	                   tt=t.i)
  #Extract MLEs and AIC
    remles        <- c(exp(optim.out$par[2:3]))
    lnL.hat     <- - optim.out$value[1]
    AIC         <- -2*lnL.hat + 2*2 #where 2 = length(REMLEs)... 
	
    out         <- list(remles=remles, 
                        lnL.hat = lnL.hat, 
                        AIC=AIC)
	return(out)
}
```


## OUSS-MLE

The deterministic discrete-time GSS model is defined as 

$$
N_t = N_{t-1} \exp(a+b \ln{N_{t-1}})
$$
where $N_t$ is the population abundance at time $t$, while $a$ and $b$ are constants representing the population growth rate and the density dependence, respectively. The stochastic model include an environmental component of noise:

$$
\begin{array}{ccc}
N_t = N_{t-1} \exp(a+b \ln{N_{t-1}}+E_t) \\
E_t \sim \text{Normal}(0,\sigma^2) \\
\end{array}
$$
where $\sigma^2$ is the variation of the process noise (stochasticity). This state-space model link the  ecological process with the observation process by log-transforming the abundance ($X_t = \ln{N_t}$), defining

$$
\begin{array}{ccc}
X_t = a + c X_{t-1}+E_t \\

Y_t = X_t + F_t \\
F_t \sim \text{Normal}(0,\tau^2)
\end{array}
$$
where $c = b+1$ (strength of density dependence) and $\tau^2$ is the variation of the observation process $F_t$. Note that if $b = 0$, $c=1$ and the model mutates to a special case of the GSS, the density independence model (***D_EM***). 

The log-transformation of the GSS model allow to estimate infinitesimal mean and variance in a continuous form. The deterministic skeleton is defined by the ordinary differential equation:

$$
\frac{dn(t)}{dt}=\theta n(t)[\ln \kappa - \ln n(t)]
$$
where $n(t)$ is the abundance of the growing entity at time $t$, while $\theta$ is a constant measure of the speed of equilibration (a trend parameter), and $\kappa$ is a constant of abundance equilibrium (carrying capacity). The solution trajectory of the previous ODE is:

$$
n(t) = \kappa \exp[e^{-\theta t} \ln{n_0/\kappa}]
$$

The stochastic version of this diffusion process from the GSS model is defined as the stochastic differential equation:

$$
dN(t) = \theta N(t)[\ln{\kappa}-\ln{N(t)}]dt+\beta N(t)dW(t)
$$
where $dW(t)$ is a random perturbation representing the environmental stochasticity or process noise, with mean $0$ and variance $dt$, with intensity of noise scaled by the term $\beta N(t)$, with $\beta >0$.

Under this diffusion process, the infinitesimal mean function is given by $m(n) = \theta n(\ln{\kappa}-\ln{n})$, and infinitesimal variance function $v(n) = \beta^2 n^2$. As $X(t) = \ln{N_t}$, the infinitesimal mean ($m_X$) and variance ($v_X$) are given by the Itô log-transformation as (***Eq4*** & ***Eq5*** in ***DP_E***):

$$
\begin{array}{ccc}
m_X(x) &=& \theta(\mu - x) \\
v_X(x) &=& \beta^2 \\
\end{array}
$$
where $\mu = \ln{\kappa}-\frac{\beta^2}{2\theta}$.

The model is defined as the stochastic differential equation (***Eq6*** in ***DP_E***)

$$
dX(t) = \theta[\mu-X(t)]dt+\beta dW(t)
$$

Estimated or observed abundance $Y(t_i)$ of the logarithmic population abundance $X(t_i)$ has an observation error $F_i$ with mean $0$ and variance $\tau^2$:

$$
\begin{array}{ccc}
Y(t_i) = X(t_i) + F_i; \\
F_i \sim \text{Normal}(0,\tau^2)
\end{array}
$$


The OUSS model have a joint multivariate normal distribution that in its stationary form have four unknown parameters: $\mu$ (in stationary distribution is the expected value of stationary distribution of log-abundance mean; it is different than $\mu_{91}$ in ***D_91EM***, named here the trend parameter $\theta$ in *1.3.2. EGSS*), $\theta$ (rate to approach to stationarity), $\beta^2$ (variability of process noise), and $\tau^2$ (variability of sampling). The relationship of these parameters with the discrete GSS model is given by the transformations:

$$
\begin{array}{ccc}
a &=& \mu(1- e^{-\theta}) \\
c &=& e^{-\theta} \\
\sigma^2 &=& \frac{(1-e^{-2\theta})\beta^2}{2\theta} \\
\tau^2 &=& \tau^2 \\
&...&\\
\mu &=& \frac{a}{1-c} \\
\theta &=& -\ln{c} \\
\beta^2 &=& -\frac{2\sigma^2 \ln{c}}{1-c^2} \\
\tau^2 &=& \tau^2 \\
\end{array}
$$

We might assume that the population have become stationary at the moment of first sampling, fluctuating around some equilibrium ($\mu$). Thus, the initial log-abundance of the population $X(0)$ is part of the stationary normal distribution (log-normal distribution of the abundance, recall $X(0)=\ln(N(0))$), and its multivariate normal log-likelihood for the stationary OUSS model is given by (see ***Eq. 19*** ***DP_E***).

$$
\ln{L}(\mu, \theta, \beta^2, \tau^2) = -\frac{(q+1)}{2}\ln(2\pi)-\frac{1}{2}\ln(|\mathbf{V}|)-\frac{1}{2}(\mathbf{y}-\mathbf{m})'\mathbf{V}^{-1}(\mathbf{y}-\mathbf{m})
$$

where $q$ is the number of time-series transitions (thus, $q+1$ reflect the length of the time-series, with the initial population estimation $y_0$ as a realized value of the random variable $Y(0)$), $\mathbf{V}$ is the variance-covariance matrix (with diagonal computed from $V[Y(t_i)] = \tau^2+\frac{\beta^2}{2\theta}$; ***Eq. 17*** in ***DP_E***), $\mathbf{y}$ is the data values ($y_0$, $y_1$, $y_2$, ..., $y_q$), and $\mathbf{m}$ is the vector of same $\mu$ in all $q+1$ times ($E[Y(t_i)] = \mu]$; ***Eq. 16*** in ***DP_E***).

This function requires three arguments for the numerical optimization in `R`:

1.  A vector of time-series of log-observed abundances `yt` ($\mathbf{y}$)
2.  A vector of observation times `tt`
3.  A vector of parameters `fguess` (a first guess for the **four** parameters), that could be roughly computing by provide the vector of log-abundance observations `yt` ($\mathbf{y}$) and the vector of observation times `tt` with the function `guess_ouss()`

```{r}
guess_ouss <- function(yt,tt){
	
  # Time-vector starting in 0.
    t.i     <- tt-tt[1];   
	# Number of time-series transitions
    q       <- length(yt)-1;   
  # length of time-series
    qp1     <- q+1;
  # time intervals
    t.s     <- t.i[2:qp1]-t.i[1:q];  
  # mean of the observations as assumed to arise from stationary distribution
    Ybar    <- mean(yt);
  # Variance of the observations
    Yvar    <- sum((yt-Ybar)*(yt-Ybar))/q;
  # Initial mu estimate (at stationary distribution)
    mu1     <- Ybar;

  # Kludge an initial value for theta based on mean of Y(t+s) given Y(t).
    th1     <- -mean(log(abs((yt[2:qp1]-mu1)/(yt[1:q]-mu1)))/t.s);            
  # Moment estimate using stationary distribution
    bsq1    <- 2*th1*Yvar/(1+2*th1);       
  # Observation error variance, assumed as first guess as betasq=tausq.
    tsq1    <- bsq1;                         

  # What to do if initial guesses is three 0's (or NAs)? Assume arbitrary values
    three0s <- sum(c(th1,bsq1,tsq1))
	  
    if(three0s==0|is.na(three0s)){
      th1   <- 0.5;
	    bsq1  <- 0.09; 
	    tsq1  <- 0.23;}

    out1    <- c(th1,bsq1,tsq1);
	
  # What to do if initial guesses are too little? Assume arbitrary values
    if(sum(out1<1e-7)>=1){
      out1  <- c(0.5,0.09,0.23)}
    
    out     <- c(mu1,out1);
	
	return(abs(out))
}
```

The numerical optimization for the MLEs in `R` is:

```{r}
negloglike_ouss_mle <- function(yt,tt,fguess){
  
  # log-abundance stationary distribution mean (Eq10 in DP_E)
    mu        <- fguess[1];   
  # Constrains parameters theta, beta^2, and tau^2 > 0
    guess     <- exp(fguess[2:4]); 
  # speed of equilibration (Eq1 in DP_E)
    theta     <- guess[1];    
  # variability of process noise
    betasq    <- guess[2];    
  # variability of sampling
    tausq     <- guess[3];    
  # number of time-series transitions
    q         <- length(yt) - 1;
  # length of time-series
    qp1       <- q+1;         
  # Variance (Eq11 in DP_E)
    Var.inf   <- betasq/(2*theta); 
  # time intervals (not used here?)
    t.s       <- tt[2:qp1] - tt[1:q]; 
  # part of Eq18 in DP_E
    t.cols    <- matrix(rep(tt,each=qp1), 
	                     nrow=qp1,
	                     ncol=qp1, 
	                     byrow=FALSE);
  # (part of Eq18 in DP_E)
    t.rows    <- t(t.cols);    
  # (part of Eq18 in DP_E) 
    abs.diffs <- abs(t.rows-t.cols);   
  # Covariance (Eq18 in DP_E)
    V         <- Var.inf*exp(-theta*abs.diffs); 
    diag(V)   <- diag(V) + rep(tausq,qp1);
  # column vector **m** (from Eq16 in DP_E)
    mu.vec    <- rep(mu,qp1); 
	
  #note the signs change because we want here the negative log-likelihood (Eq19*-1)
    neglogl   <- (qp1/2)*log(2*pi) + (1/2)*log(det(V)) + (1/2)*(yt-mu.vec)%*%ginv(V)%*%(yt-mu.vec);
	
  return(neglogl)
	}
```

Now we can compute the MLE of the OUSS model for stationary cases with the function `ouss_mle()`:

```{r}
ouss_mle <- function(yt,tt,fguess){
	
  # Time-vector starting in 0.
	  t.i         <- tt-tt[1];
	# Number of time-series transitions
	  q           <- length(yt)-1;
	# length of time-series
	  qp1         <- q+1;
	# initial guesses (mu as estimated, theta, betasq, and tausq at log scale (?))
	  guess.optim <- c(fguess[1], log(fguess[2:4])) 
	# numerical optimization
	  optim.out   <- optim(par=guess.optim, 
	                   fn=negloglike_ouss_mle, 
	                   method="Nelder-Mead", 
	                   yt=yt, 
	                   tt=t.i)
	#Extract MLEs and AIC
	  mles        <- c(optim.out$par[1], 
	                   exp(optim.out$par[2:4]))
	  lnL.hat     <- - optim.out$value[1]
	  AIC         <- -2*lnL.hat + 2*4 #where 4 = length(MLEs)... 
	
	out           <- list(mles=mles, 
	                      lnL.hat = lnL.hat, 
	                      AIC=AIC)
	return(out)
}
```

## OUSS-REMLE

The restricted maximum likelihood estimation (REMLE) eliminates the parameter of the mean vector ($\mu$) and leave only parameters in the variance-covariance matrix (***DP_E***). This has better statistical properties than MLE for the GSS and EGSS models. The multivariate normal log-likelihood for the REMLE stationary OUSS model is given by (**Eq. 22** in ***DP_E***)

$$
\ln{L}(\theta, \beta^2, \tau^2) = -\frac{q}{2}\ln(2\pi)-\frac{1}{2}\ln(|\mathbf{\Phi}|)-\frac{1}{2}\mathbf{w}'\mathbf{\Phi}^{-1}\mathbf{w}
$$

This function requires three arguments for the numerical optimization in `R`:

1.  A vector of time-series of log-observed abundances `yt` ($\mathbf{y}$)
2.  A vector of observation times `tt`
3.  A vector of the **three** unknown parameters (`fguess[2:4]` from the `guess_ouss()`)

```{r}
negloglike_ouss_remle=function(yt,tt,fguess){
	# Constrains parameters theta, beta^2, and tau^2 > 0
	
  # speed of equilibration (Eq1 in DP_E)
	  theta         <- exp(fguess[2]);    
	# variability of process noise
	  betasq        <- exp(fguess[3]);    
	# variability of sampling
	  tausq         <- exp(fguess[4]);    
	# number of time-series transitions
	  q             <- length(yt) - 1;
	# length of time-series
	  qp1           <- q+1;         
	# Variance (Eq11 in DP_E)
	  Var.inf       <- betasq/(2*theta); 
	# time intervals (not used here?)
	  t.s           <- tt[2:qp1] - tt[1:q]; 
	# part of Eq18 in DP_E
	  t.cols        <- matrix(rep(tt,each=qp1),
	                          nrow=qp1,
	                          ncol=qp1,
	                          byrow=FALSE);
	# (part of Eq18 in DP_E)
	  t.rows        <- t(t.cols);    
	# (part of Eq18 in DP_E) 
	  abs.diffs     <- abs(t.rows-t.cols);  
  
	# Covariance of the process (Eq18 in DP_E)
	  Sigma.mat     <- Var.inf*exp(-theta*abs.diffs);
  # Create a matrix full of 0s of the length of time series
	  Itausq        <- matrix(0,qp1,qp1);
  # Repeat the observation error variance guess in the diagonal of the matrix
	  diag(Itausq)  <- rep(tausq,qp1);
  # add Covariance with the matrix
	  V             <- Sigma.mat+Itausq;
	# Create the differencing matrix **D**
    Dmat          <- cbind(-diag(1,q),matrix(0,q,1)) + cbind(matrix(0,q,1),diag(1,q));
  # Variance-covariance matrix **Phi** (Eq20 DP_E)
    Phi.mat       <- Dmat%*%V%*%t(Dmat); 
  # simple differencing of the observations (W_i? )
    wt            <- yt[2:qp1]-yt[1:q];
  
	# note the signs change because we want here the negative log-likelihood (Eq22*-1)
  neglogl         <- (q/2)*log(2*pi) + (1/2)*log(det(Phi.mat)) + (1/2)*wt%*%ginv(Phi.mat)%*%wt; 
   
  # What to do if the `neglogl` is not finite? assign a big number of 50000
   if(is.infinite(neglogl)==TRUE){
     return(50000)}else{
       return(neglogl)}
}
```

Now we can compute the REMLE of the OUSS model for stationary cases with the function `ouss_remle()`

```{r}
ouss_remle <- function(yt, tt, fguess){

  # Time-vector starting in 0.
	  t.i           <- tt-tt[1];
	# Number of time-series transitions
	# length of time-series
	  q             <- length(yt)-1;
	  qp1           <- q+1;
	# time intervals
    t.s           <- t.i[2:qp1]-t.i[1:q];  
  # initial guesses (all, but negloglike.OU.remle will use only fguess[2:4])
    guess.optim   <- c(fguess[1], 
                       log(fguess[2:4])); 
	# numerical optimization
    optim.out     <- optim(par = guess.optim,
                           fn=negloglike_ouss_remle,
                           method="Nelder-Mead",
                           yt=yt,
                           tt=t.i);
	# Restricted maximum likelihood estimates (REMLE) and lnL.hat
    remles        <- exp(optim.out$par);
	  theta.remle   <- remles[2];
	  betasq.remle  <- remles[3];
	  tausq.remle   <- remles[4];

	  lnL.hat       <- -optim.out$value[1];
	
	# Variance (Eq11 in DP_E)
	  Var.inf       <- betasq.remle/(2*theta.remle)
	# creates an matrix full of 1 dim qp1 x qp1
	  vx            <- matrix(1,qp1,qp1);
	# iterate to fill the matrix (couldn't find vx in DP_E!)
	  for (t.i in 1:q){
   		vx[(t.i+1):qp1,t.i]=exp(-theta.remle*cumsum(t.s[t.i:q]));
   		vx[t.i,(t.i+1):qp1]=vx[(t.i+1):qp1,t.i];
	  }
	# ?
	  Sigma.mat     <- vx*Var.inf;
	# Create a matrix full of 0s of the length of time series
	  Itausq        <- matrix(0,qp1,qp1);
	# Repeat the observation error variance remle in the diagonal of the matrix
		diag(Itausq)  <- rep(tausq.remle,qp1);
	# Variance-covariance matrix (V.hat) evaluated with remles to estimate mu.hat
	  V.remle       <- Sigma.mat+Itausq;
	# column vector matrix of ones
	  j             <- matrix(1,qp1,1);
	# Inverse matrix (part of Eq23 in DP_E)
	  Vinv          <- ginv(V.remle);
	# REMLE of mu (mu.hat) with Eq23 in DP_E
	  mu.remle      <- (t(j)%*%Vinv%*%yt)/(t(j)%*%Vinv%*%j);
  #AIC
	  AIC           <- -2*lnL.hat + 2*4 #where 4 = length(mles)... 
	
	#Results
	  out           <- list(remles = c(mu.remle, 
	                                   theta.remle, 
	                                   betasq.remle, 
	                                   tausq.remle),
	                        lnLhat = lnL.hat,
	                        AIC = AIC)
	return(out)	
}
```

## OUSS-MLE - Nonstationary

For populations experiencing transient growth phases, we can not assume that the first observations arise from stationary distributions (*e.g.*, during colonization of new areas, translocations, or recovery after catastrophic events). We can apply the multivariate normal log-likelihood, but the element in the mean vector $\mathbf{m}$ correspond to the data value $y_i$ (from the infinitesimal Itô trasnformation) given by $E[Y(t_i)] = \mu-(\mu-x_0)\exp(-\theta t_i)$ (***Eq. 13*** in ***DP_E***) instead of $E[Y(t_i)] = \mu$ (***Eq. 16*** in ***DP_E***), here we denote this new vector $\mathbf{u}$. In addition, the variance is $V[Y(t_i)]=\frac{\beta^2}{2\theta}(1-\exp(-2\theta t_i))+\tau^2$ (***Eq. 14*** in ***DP_E***) instead of $V[Y(t_i)]=\frac{\beta^2}{2\theta}+\tau^2$ (***Eq. 17*** in ***DP_E***). The log-likelihood must be numerically maximized jointly for the five unknown parameters: $\mu$ (the trend parameters for the log-abundance mean, no stationary), $\theta$ (rate to approach to stationarity), $\beta^2$ (variability of process noise), $\tau^2$ (variability of sampling), and $x_0$ (the initial population, named $\beta$ by ***H_O***).

$$
\ln{L}(\mu, \theta, \beta^2, \tau^2, x_0) = -\frac{(q+1)}{2}\ln(2\pi)-\frac{1}{2}\ln(|\mathbf{V}|)-\frac{1}{2}(\mathbf{y}-\mathbf{u})'\mathbf{V}^{-1}(\mathbf{y}-\mathbf{u})
$$

For the numerical optimization, we will need three arguments:

1.  A vector of time-series of log-observed abundances `yt` ($\mathbf{y}$)
2.  A vector of observation times `tt`
3.  A vector of initial parameters `fguess_NoSt` (a first guess for the five parameters), that could be roughly computing by provide the vector of log-abundance observations `yt` ($\mathbf{y}$) and the vector of observation times `tt` with the function `guess_ouss_NoSt()`:

```{r}
guess_ouss_NoSt <- function(yt,tt){
	
  # Time-vector starting in 0.
    t.i     <- tt-tt[1];   
  # Number of time-series transitions
    q       <- length(yt)-1;   
  # length of time-series
    qp1     <- q+1;
  # time intervals
    t.s     <- t.i[2:qp1]-t.i[1:q];  
  # mean of the observations as assumed to arise from stationary distribution
    Ybar    <- mean(yt);
  # Variance of the observations
    Yvar    <- sum((yt-Ybar)*(yt-Ybar))/q;
	  
  # Initial population - borrowed from EGSS (if we now exactly x_0 = y_0)
  
  # mean of the time series
    Tbar    <- mean(t.i)
  # trend parameter for EGOE (r = ln(lambda))
    r.egoe  <- sum((t.i-Tbar)*(yt-Ybar))/sum((t.i-Tbar)*(t.i-Tbar));
  # Initial population  of EGOE
    x0.egoe <- Ybar-r.egoe*Tbar
  # Initial population  of EGPN is the first observation
    x0.egpn <- yt[1];
  # Initial population
    x0.out  <- (x0.egoe+x0.egpn)/2;
	  
  # Kludge an initial value for theta based on mean of Y(t+s) given Y(t).
    th1     <- -mean(log(abs((yt[2:qp1]-Ybar)/(yt[1:q]-Ybar)))/t.s);            
  # Moment estimate using stationary distribution
    bsq1    <- 2*th1*Yvar/(1+2*th1);       
  # Observation error variance, assumed as first guess as betasq=tausq.
    tsq1    <- bsq1;                         

  # What to do if initial guesses is three 0's (or NAs)? Assume arbitrary values
    three0s <- sum(c(th1,bsq1,tsq1))
	  
    if(three0s==0|is.na(three0s)){
	    th1   <- 0.5;
	    bsq1  <- 0.09; 
	    tsq1  <- 0.23;}
	  
    out1    <- c(th1,
                 bsq1,
                 tsq1,
                 x0.out);
	
  # What to do if initial guesses are too little? Assume arbitrary values
    if(sum(out1<1e-7)>=1){
      out1  <- c(0.5, 
                 0.09, 
                 0.23, 
                 1.6)}
	
  # Expected value of Y(t)
    mu1     <- Ybar - (Ybar-x0.out)*exp(-th1*Tbar)
	  
    out     <- c(mu1,
                 out1);
	
	return(abs(out))
}
```

The numerically jointly maximization for the five unknown parameters $\mu$, $\theta$, $\beta^2$, $\tau^2$, and $x_0$ is

```{r}
negloglike_ouss_mle_NoSt <- function(yt,tt,fguess_NoSt){
		
  # from the mu initial guess for nonstationary (guess.ouss.NoSt)
    mu          <- fguess_NoSt[1];
  # Constrains parameters theta, beta^2, and tau^2 > 0
    guess       <- exp(fguess_NoSt[2:4]); 
  # speed of equilibration (Eq1 in DP_E)
    theta       <- guess[1];    
  # variability of process noise
    betasq      <- guess[2];    
  # variability of sampling
    tausq       <- guess[3];  
  # initial population (initial guess)
    x0          <- fguess_NoSt[5];
  # number of time-series transitions
    q           <- length(yt) - 1;
  # length of time-series
    qp1         <- q+1;         
  # Time-vector starting in 0.
    t.i         <- tt-tt[1];
	 
  # vector u, from Eq13 in DP_E
    u           <- rep(0,qp1)
    for (t in 0:qp1){
	    u[t]      <- mu - (mu-x0)*exp(-theta*t)
	  }
  # Variance iterate with t (Eq14 in DP_E)
    Var.inf     <- rep(0,qp1)
	  for (t in 0:qp1){
	    Var.inf[t] <- (betasq/(2*theta))*(1-exp(-2*theta*t))+tausq
	  }
	  
  #time intervals (not used here?)
    t.s       <- tt[2:qp1] - tt[1:q]; 
	
  # part of Eq15 in DP_E
    t.cols    <- matrix(rep(tt,each=qp1),
                        nrow=qp1,
                        ncol=qp1,
                        byrow=FALSE);
  # (part of Eq15 in DP_E)
    t.rows    <- t(t.cols);    
	# (part of Eq15 in DP_E) 
	  abs.diffs <- abs(t.rows-t.cols); 
	# save minima of input values in the matrices
	  min.titj  <- pmin(t.rows,t.cols);
	# Covariance (Eq15 in DP_E)
	  V         <- (betasq/(2*theta))*(1-exp(-2*theta*min.titj))*exp(-theta*abs.diffs); 
	  diag(V)   <- diag(V) + rep(tausq,qp1);

	#note the signs change because we want here the negative log-likelihood (Eq19*-1)
	  neglogl   <- (qp1/2)*log(2*pi) + (1/2)*log(det(V)) + (1/2)*(yt-u)%*%ginv(V)%*%(yt-u);
	
	return(neglogl)	
}
```

Now we can compute the MLE of the OUSS model for nonstationary cases and unknown initial population with the function `ouss_mle_NoSt()`

```{r}
ouss_mle_NoSt <- function(yt,tt,fguess_NoSt){
	
  # Time-vector starting in 0.
    t.i         <- tt-tt[1];
  # Number of time-series transitions
    q           <- length(yt)-1;
  # length of time-series
    qp1         <- q+1;
  # initial guesses (mu and x0 as estimated,while theta, betasq, and tausq at log scale (?))
    guess.optim <- c(fguess_NoSt[1], 
                     log(fguess_NoSt[2:4]),
                     fguess_NoSt[5]) 
  # numerical optimization
    optim.out   <- optim(par=guess.optim, 
	                   fn=negloglike_ouss_mle_NoSt, 
	                   method="Nelder-Mead", 
	                   yt=yt, 
	                   tt=t.i)
  #Extract MLEs and AIC
    mles        <- c(optim.out$par[1], 
                     exp(optim.out$par[2:4]), 
                     optim.out$par[5])
    lnL.hat     <- - optim.out$value[1]
    AIC         <- -2*lnL.hat + 2*5 #where 5 = length(MLEs)... 
	
    out         <- list(mles=mles, 
                        lnL.hat=lnL.hat,
                        AIC=AIC)
  return(out)
}
```

If by any chance we know the initial population (as in translocations of a known number of individuals), we can fix the parameter $x_0$, and the observation of that initial population represent the ecological process estimation ($y_0 = x_0$). In this case, the MLE uses the same multivariate normal log-likelihood, but applied to just $q$ observations (because we do not include $y_0$) recorded at times $t_1$, $t_2$, ..., $t_q$. 

$$
\ln{L}(\mu, \theta, \beta^2, \tau^2) = -\frac{q}{2}\ln(2\pi)-\frac{1}{2}\ln(|\mathbf{V}|)-\frac{1}{2}(\mathbf{y}-\mathbf{u})'\mathbf{V}^{-1}(\mathbf{y}-\mathbf{u})
$$


## Parametric bootstrap functions

The parametric bootstrapping provide confidence intervals of the parameters. ***DP_E*** recommended 2000 simulations or more. In addition, this technique allows predict values in missing times of the time-series. More relevant, we can simulate trends from fitted SS models of different versions for conducting risk-based population viability monitoring (VPM). We will require the following functions:

### 3.7.1. Multivariate Normal random number generator

To generate random numbers from a multivariate normal distribution. It requires: 
1. `n` = the number of random samples of a multivariate normal vector 
2. $\mu$ = mean vector of the multivariate normal distribution to sample from 
3. `cov.mat` = Variance-covariance matrix of the multivariate normal distribution to sample from

```{r}
randmvn <- function(n, mu.vec, cov.mat){
	
  # Save the length of the mean vector of the multivariate normal distribution to sample
    p         <- length(mu.vec);
  # The Cholesky decomposition (factorization of a real symmetric positive-definite sqr matriz)
    Tau       <- chol(cov.mat, pivot=TRUE);
  # generate normal deviates outside loop
    Zmat      <- matrix(rnorm(n=p*n,mean=0,sd=1),nrow=p,ncol=n); 
	
  # empty matrix
    out       <- matrix(0,nrow=p,ncol=n);
  # iterate
    for(i in 1:n){
      Z       <- Zmat[,i];
      out[,i] <- t(Tau)%*%Z + mu.vec
		}
	
  return(out)
}
```

### 3.7.2. Simulation functions

To simulate ≥2000 data sets for EGSS, OUSS.St, and OUSS.NoSt models fitted. First, lets do EGSS, which requires:

1.  The number of bootstrap replicates to simulate (≥2000) `nsims` 
2.  The ORIGINAL vector of observation times `tt` ($t_0$, $t_1$, $t_2$, ..., $t_q$)
3.  A vector of parameters values estimated from `egss_mle()` ($\hat{\theta}$, $\hat{\sigma^2}$, $\hat{\tau^2}$, $\hat{x_0}$; `egss$mles`) 

```{r}
egss_sim <- function(nsims,tt,parms){
	
  # Time-vector starting in 0.
    t.i           <- tt-tt[1];   
  # Number of time-series transitions
    q             <- length(t.i)-1;   
  # length of time-series
    qp1           <- q+1;
    
  # parameters
    theta         <- parms[1];
    sigmasq       <- parms[2];
    tausq         <- parms[3];
    x0            <- parms[4];

    vx      <- matrix(0,qp1,qp1);
    for(i in 1:q){ 
		  vx[((i+1):qp1),((i+1):qp1)] <- matrix(1,(qp1-i),(qp1-i))*t.i[(i+1)];
	  } 
    
    Sigma.mat     <- sigmasq*vx;
    Itausq        <- matrix(rep(0,(qp1*qp1)), 
                            nrow=qp1, 
                            ncol=qp1);
    diag(Itausq)  <- rep(tausq,qp1);
    V             <- Sigma.mat + Itausq;
    theta.vec     <- matrix((x0+theta*t.i), 
                            nrow=qp1,
                            ncol=1);
    out           <- randmvn(n=nsims,
                             mu.vec=theta.vec,
                             cov.mat=V);

  return(out)
}
```

Now let creates the function for the OUSS, which requires: 

1.  The number of bootstrap replicates to simulate (≥2000) `nsims` 
2.  The ORIGINAL vector of observation times `tt` ($t_0$, $t_1$, $t_2$, ..., $t_q$)
3.  A vector of parameters values estimated from `ouss_remle()` ($\hat{\mu}$, $\hat{\theta}$, $\hat{\beta^2}$, $\hat{\tau^2}$; `ouss.st$remles`) 

```{r}
ouss_sim <- function(nsims,tt,parms){

  # Time-vector starting in 0.
    t.i       <- tt-tt[1];   
  # Number of time-series transitions
    q         <- length(t.i)-1;   
  # length of time-series
    qp1       <- q+1;
    
  # parameters
    mu        <- parms[1];
    theta     <- parms[2]; 
    betasq    <- parms[3];
    tausq     <- parms[4];

    Var.inf   <- betasq/(2*theta); 
    t.s       <- t.i[2:qp1] - t.i[1:q];
    t.cols    <- matrix(rep(t.i,each=qp1),
                        nrow=qp1,
                        ncol=qp1,
                        byrow=FALSE);
    t.rows    <- t(t.cols);
    abs.diffs <- abs(t.rows-t.cols);
    V         <- Var.inf*exp(-theta*abs.diffs);
    diag(V)   <- diag(V) + rep(tausq,qp1);
    m.vec     <- rep(mu,qp1);
    out       <- randmvn(n=nsims, 
                         mu.vec=m.vec,
                         cov.mat = V)
	return(out)
}
```

And finally, for the OUSS.NoSt requires: 

1.  The number of bootstrap replicates to simulate (≥2000) `nsims` 
2.  The ORIGINAL vector of observation times `tt` ($t_0$, $t_1$, $t_2$, ..., $t_q$)
3.  A vector of parameters values estimated from `ouss_mle_NoSt()` ($\hat{\mu}$, $\hat{\theta}$, $\hat{\beta^2}$, $\hat{\tau^2}$, $\hat{x_0}$; `ouss.NoSt$mles`) 

```{r}
ouss_sim_NoSt <- function(nsims,tt,parms){

  # Time-vector starting in 0.
    t.i         <- tt-tt[1];   
  # Number of time-series transitions
    q           <- length(t.i)-1;   
  # length of time-series
    qp1         <- q+1;
    
  # parameters
  	mu          <- parms[1];
  	theta       <- parms[2]; 
  	betasq      <- parms[3];
  	tausq       <- parms[4];
  	x0          <- parms[5];  

  # Variance iterate with t (Eq14 in DP_E)
    Var.inf     <- rep(0,qp1)
    for (t in 0:qp1){
      Var.inf[t]<- (betasq/(2*theta))*(1-exp(-2*theta*t))+tausq
	  }
	
    t.s         <- t.i[2:qp1] - t.i[1:q];
    t.cols      <- matrix(rep(t.i,each=qp1),
                          nrow=qp1,
                          ncol=qp1,
                          byrow=FALSE);
  	t.rows      <- t(t.cols);
  	abs.diffs   <- abs(t.rows-t.cols);
  	min.titj    <- pmin(t.rows,t.cols);
  	V           <- (betasq/(2*theta))*(1-exp(-2*theta*min.titj))*exp(-theta*abs.diffs); 
    diag(V)     <- diag(V) + rep(tausq,qp1);
  # vector u, from Eq13 in DP_E
    u           <- rep(0,qp1)
    for (t in 0:qp1){
      u[t] <- mu - (mu-x0)*exp(-theta*t)
	  }

    out         <- randmvn(n=nsims,
                           mu.vec=u,
                           cov.mat=V)
	return(out)
}
```

### 3.7.3. Predicted trajectory of unobserved process (OAC - Only OUSS.St)

This function(s) will predict the trajectory of unobserved process (abundance).

##### EGSS predicted trajectory (Almost there, not yet)

For EGSS it requires (in theory... **NOT READY**, yet):

1.  A vector of time-series of log-observed abundances `yt` ($\mathbf{y}$)
2.  A vector of observation times `tt`
3.  A vector of parameters values estimated from `eggs_mle()` ($\hat{\theta}$, $\hat{\sigma^2}$, $\hat{\tau^2}$, $\hat{x_0}$) 

```{r eval=F}
egss_predict <- function(yt,tt,parms){
	
  # Time-vector starting in 0.
	  t.i     <- tt-tt[1];   
	# Number of time-series transitions
	  q       <- length(t.i)-1;   
	# length of time-series
    qp1     <- q+1;
    t.s     <- t.i[2:qp1] - t.i[1:q];
    
  # parameters
    theta   <- parms[1];
	  sigmasq <- parms[2];
	  tausq   <- parms[3];
	  x0      <- parms[4];
	 
	# Missing t.s   
	  nmiss   <- t.s-1;
	  long.nmiss <- c(0,nmiss);
	  Nmiss   <- sum(nmiss)

	  vx      <- matrix(0,qp1,qp1);
	  for(i in 1:q){ 
		  vx[((i+1):qp1),((i+1):qp1)] <- matrix(1,(qp1-i),(qp1-i))*t.i[(i+1)];
	  } 
	Sigma.mat    <- sigmasq*vx;
	Itausq       <- matrix(rep(0,(qp1*qp1)), 
	                       nrow=qp1, 
	                       ncol=qp1);
	diag(Itausq) <- rep(tausq,qp1);
	V            <- Sigma.mat + Itausq;
	
	D1mat=cbind(-diag(1/t.s),
	            matrix(0,q,1))+cbind(matrix(0,q,1),
	                                 diag(1/t.s));
	
	V1mat=D1mat%*%V%*%t(D1mat); 
	
	W.t=(yt[2:qp1]-yt[1:q])/t.s;
	j1=matrix(1,q,1); 
	V1inv=ginv(V1mat);
	
	theta.remle=(t(j1)%*%V1inv%*%W.t)/(t(j1)%*%V1inv%*%j1); 
	j=matrix(1,qp1,1); 
	Vinv=ginv(V);
	x0.remle=(t(j)%*%Vinv%*%(yt-c(theta.remle)*t.i))/(t(j)%*%Vinv%*%j);
	Var_theta.remle=1/(t(j1)%*%V1inv%*%j1); # Variance of theta 
	theta_hi.remle=theta.remle+1.96*sqrt(Var_theta.remle); # 95% CI for theta 
	theta_lo.remle=theta.remle-1.96*sqrt(Var_theta.remle)
	
	#Calculate estimated population size for EGSS model
	
	m=rep(1,qp1); # Will contain Kalman means for Kalman calculations.
	v=rep(1,qp1); # Will contain variances for Kalman calculations.
	
	m[1]=x0; # Initial mean of Y(t). 
	v[1]=tausq; # Initial variance of Y(t). 
	
	for (ti in 1:q) # Loop to generate estimated population abundances 
	  { # using Kalman filter (see equations 6 & 7, # Dennis et al. (2006)).
	  m[ti+1]=theta+(m[ti]+((v[ti]-tausq)/v[ti])*(yt[ti]-m[ti]));
	  v[ti+1]=tausq*((v[ti]-tausq)/v[ti])+sigmasq+tausq; 
	  } 
	
	# The following statement calculates exp{E[X(t) | Y(t), Y(t-1),...,Y(0)]};
	# see equation 54 in Dennis et al. (2006).
	
	Predict.t=exp(m+((v-tausq)/v)*(yt-m));
	
	return(data.frame(Time.t = tt, REMLE = Predict.t, Observed.y = exp(yt)))
}
```

##### OUSS predicted trajectory

For the OUSS in stationary dynamic, the function requires: 

1.  A vector of time-series of log-observed abundances `yt` ($\mathbf{y}$)
2.  A vector of observation times `tt`
3.  A vector of parameters values estimated from `OUSS.St.MLE()` ($\hat{\mu}$, $\hat{\theta}$, $\hat{\beta^2}$, $\hat{\tau^2}$) 
4.  A logical argument for simple plotting (`T`/`F`)

```{r}
ouss_predict <- function(yt,tt,parms, plot.it="TRUE"){

	  t.i             <- tt-tt[1];   
	  q               <- length(t.i)-1;   
    qp1             <- q+1;
    
  # parameters
	  mu              <- parms[1];
	  theta           <- parms[2]; 
	  betasq          <- parms[3];
	  tausq           <- parms[4];

	  Var.inf         <- betasq/(2*theta); 
	  t.s             <- t.i[2:qp1] - t.i[1:q];
	  t.cols          <- matrix(rep(t.i,each=qp1),nrow=qp1,ncol=qp1, byrow=FALSE);
	  t.rows          <- t(t.cols);
	  abs.diffs       <- abs(t.rows-t.cols);

	  nmiss           <- t.s-1;
	  long.nmiss      <- c(0,nmiss);
	  Nmiss           <- sum(nmiss)

	  long.t          <- t.i[1]:max(t.i)
	  where.miss      <- which(is.na(match(x=long.t,table=t.i)),
	                           arr.ind=TRUE)
	  lt.cols         <- matrix(rep(long.t),
	                            nrow=(qp1+Nmiss),
	                            ncol=(qp1+Nmiss),
	                            byrow=FALSE);
	  lt.rows         <- t(lt.cols);
	  labs.diffs      <- abs(lt.rows-lt.cols);
	
	  Sigma.mat       <- Var.inf*exp(-theta*abs.diffs);
	  Itausq          <- matrix(0,qp1,qp1);
	  diag(Itausq)    <- rep(tausq,qp1);
	  V               <- Sigma.mat+Itausq;

	  long.V          <- Var.inf*exp(-theta*labs.diffs) + diag(rep(tausq,(qp1+Nmiss)))

	  Predict.t       <- rep(0,qp1);
	  Muvec           <- rep(mu,q);
	  miss.predict    <- list()
	  Muvec.miss      <- rep(mu,qp1);
	  start.miss      <- 1
	  stop.miss       <- 0
    for (tj in 1:qp1){
		  Y.omitj       <- yt[-tj];    #  Omit observation at time tj.
		  V.omitj       <- V[-tj,-tj];  #  Omit row tj and col tj from var-cov matrix.
		  V12           <- V[tj,-tj];       #  Submatrix:  row tj without col tj.
		  Predict.t[tj] <- mu+V12%*%ginv(V.omitj)%*%(Y.omitj-Muvec);  #  Graybill's 1976 Thm.
		
    if(long.nmiss[tj]==0){
      miss.predict[[tj]] <- Predict.t[tj]}else 
		    if(long.nmiss[tj]>0){

			    start.miss <- stop.miss+1
			    ntjmiss    <- long.nmiss[tj]
			    mu.miss    <- rep(mu,ntjmiss);
			    ind.tjmiss <- where.miss[start.miss:(start.miss+(ntjmiss-1))]
			    stop.miss  <- stop.miss+ntjmiss
	
			    longV12    <- long.V[ind.tjmiss,-where.miss]		
			
			    miss.predict[[tj]] <- c(mu.miss + longV12%*%ginv(V)%*%(yt-Muvec.miss), Predict.t[tj])
		      }
	     }

	Predict.t <- exp(Predict.t);
	LPredict.t <- exp(as.vector(unlist(miss.predict)))

	isinf <- sum(is.infinite(Predict.t))
	if(isinf>0){
	  where.infs <- which(is.infinite(Predict.t)==TRUE, arr.ind=TRUE)
	  Predict.t[where.infs] <- .Machine$double.xmax
	}

	isinf2 <- sum(is.infinite(LPredict.t))
	if(isinf2>0){
	  where.infs <- which(is.infinite(LPredict.t)==TRUE, arr.ind=TRUE)
	  LPredict.t[where.infs] <- .Machine$double.xmax
	}
	
		if(plot.it=="TRUE"){	
		#  Plot the data & model-fitted values
		#X11()
		plot(tt,exp(yt),xlab="Time",ylab="Population abundance",type="b",cex=1.5, 
			main="Predicted (--) and observed (-o-) abundances");#  Population data are circles.
		par(lty="dashed"); #  Predicted abundances are dashed line.
		points(tt,Predict.t, type="l", lwd=1);
	}
	
	return(list(cbind(tt,Predict.t,exp(yt)), cbind(long.t,LPredict.t) ))
}
```

Just to compare the trajectories, lets do two examples. First, with the example data:

```{r}
#a vector of parameters
REMLEstimates.ouss <- ouss_remle(yt = yt1, tt = tt1, guess_ouss(yt = yt1, tt = tt1))
ouss_predict(yt = yt1, tt = tt1, REMLEstimates.ouss$remles, plot.it = T)
```

and other with data removed:

```{r}
#a vector of parameters
REMLEstimates.ouss.gap <- ouss_remle(yt = yt2, tt = tt2, guess_ouss(yt = yt2, tt = tt2))
ouss_predict(yt = yt2, tt = tt2, REMLEstimates.ouss.gap$remles, plot.it = T)

REMLEstimates.egss.gap <- egss_mle(yt = yt2, tt = tt2, guess_egss(yt = yt2, tt = tt2))
predicted.egss <- egss_predict(yt = yt2, tt = tt2, REMLEstimates.egss.gap$mles)

ggplot(predicted.egss, aes(x = Time, y = Observed))+
  geom_line(color = "blue")+
  geom_point(color = "blue")+
  theme_classic() + 
  geom_line(aes(y = REMLE), color = "red")+
  geom_point(aes(y = REMLE), color = "red")

```

Even with absent of some values, it follows the trajectory. Still, we need confidence intervals (`ouss_pboot()`, below) to be sure the MLE is within the stationary distribution.

##### OUSS nonstationary dynamic predicted trajectory (NOT READY, yet)

And finally, for `OUSS_NoSt_predict()`, which requires (in theory... **NOT READY** yet): 

1.  A vector of time-series of log-observed abundances `yt` ($\mathbf{y}$)
2.  A vector of observation times `tt`
3.  A vector of parameters values estimated from `OUSS_NoSt_MLE()` ($\hat{\mu}$, $\hat{\theta}$, $\hat{\beta^2}$, $\hat{\tau^2}$, $\hat{x_0}$) 

```{r eval=F}
ouss_predict_NoSt <- function(yt,tt,parms, plot.it="TRUE"){

	  t.i         <- tt-tt[1];   
	  q           <- length(t.i)-1;   
    qp1         <- q+1;
    
  # parameters
	  mu          <- parms[1];
	  theta       <- parms[2]; 
	  betasq      <- parms[3];
	  tausq       <- parms[4];
    x0          <- parms[5];


	  Var.inf     <- betasq/(2*theta); 
	  t.s         <- t.i[2:qp1] - t.i[1:q];
	  t.cols      <- matrix(rep(t.i,each=qp1),nrow=qp1,ncol=qp1, byrow=FALSE);
	  t.rows      <- t(t.cols);
	  abs.diffs   <- abs(t.rows-t.cols);

	  nmiss       <- t.s-1;
	  long.nmiss  <- c(0,nmiss);
	  Nmiss       <- sum(nmiss)

	  long.t      <- t.i[1]:max(t.i)
	  where.miss  <- which(is.na(match(x=long.t,table=t.i)),arr.ind=TRUE)
	  lt.cols     <- matrix(rep(long.t),nrow=(qp1+Nmiss),ncol=(qp1+Nmiss), byrow=FALSE);
	  lt.rows     <- t(lt.cols);
	  labs.diffs  <- abs(lt.rows-lt.cols);
	
	  Sigma.mat   <- Var.inf*exp(-theta*abs.diffs);
	  Itausq      <- matrix(0,qp1,qp1);
	  diag(Itausq)<- rep(tausq,qp1);
	  V           <- Sigma.mat+Itausq;

	  long.V      <- Var.inf*exp(-theta*labs.diffs) + diag(rep(tausq,(qp1+Nmiss)))

	  Predict.t   <- rep(0,qp1);
	  Muvec       <- rep(mu,q);
	  miss.predict<- list()
	  Muvec.miss  <- rep(mu,qp1);
	  start.miss  <- 1
	  stop.miss   <- 0
	 for (tj in 1:qp1){
		  Y.omitj       <- yt[-tj];    #  Omit observation at time tj.
		  V.omitj       <- V[-tj,-tj];  #  Omit row tj and col tj from var-cov matrix.
		  V12           <- V[tj,-tj];       #  Submatrix:  row tj without col tj.
		  Predict.t[tj] <- mu+V12%*%ginv(V.omitj)%*%(Y.omitj-Muvec);  #  Graybill's 1976 Thm.
		
		if(long.nmiss[tj]==0){
		  miss.predict[[tj]] <- Predict.t[tj]}else 
		    if(long.nmiss[tj]>0){

			    start.miss <- stop.miss+1
			    ntjmiss <- long.nmiss[tj]
			    mu.miss <- rep(mu,ntjmiss);
			    ind.tjmiss <- where.miss[start.miss:(start.miss+(ntjmiss-1))]
			    stop.miss  <- stop.miss+ntjmiss
	
			    longV12 <- long.V[ind.tjmiss,-where.miss]		
			
			    miss.predict[[tj]] <- c(mu.miss + longV12%*%ginv(V)%*%(yt-Muvec.miss), Predict.t[tj])
		      }
	     }

	Predict.t <- exp(Predict.t);
	LPredict.t <- exp(as.vector(unlist(miss.predict)))

	isinf <- sum(is.infinite(Predict.t))
	if(isinf>0){
	  where.infs <- which(is.infinite(Predict.t)==TRUE, arr.ind=TRUE)
	  Predict.t[where.infs] <- .Machine$double.xmax
	}

	isinf2 <- sum(is.infinite(LPredict.t))
	if(isinf2>0){
	  where.infs <- which(is.infinite(LPredict.t)==TRUE, arr.ind=TRUE)
	  LPredict.t[where.infs] <- .Machine$double.xmax
	}
	
		if(plot.it=="TRUE"){	
		#  Plot the data & model-fitted values
		#X11()
		plot(tt,exp(yt),xlab="Time",ylab="Population abundance",type="b",cex=1.5, 
			main="Predicted (--) and observed (-o-) abundances");#  Population data are circles.
		par(lty="dashed"); #  Predicted abundances are dashed line.
		points(tt,Predict.t, type="l", lwd=1);
	}
	
	return(list(cbind(tt,Predict.t), cbind(long.t,LPredict.t) ))
}
```


### 3.7.4. Parametric bootstrapping (OAC - Only OUSS stationary)

Finally, we will estimate CI. For now only with the OUSS. It requires: 

1.  A vector of time-series of log-observed abundances `yt` ($\mathbf{y}$)
2.  A vector of observation times `tt`
3.  A vector of parameters values estimated from `ouss_mle()` or `ouss_remle()` ($\hat{\mu}$, $\hat{\theta}$, $\hat{\beta^2}$, $\hat{\tau^2}$) 
4.  A logical argument for simple plotting (`T`/`F`)

```{r}
ouss_pboot <- function(B=2, yt, tt, parms, REMLE="FALSE", plot.it="FALSE"){
	
  	t.i             <- tt-tt[1];   
	  q               <- length(t.i)-1;   
    qp1             <- q+1;
    
  # parameters
	  mu              <- parms[1];
	  theta           <- parms[2]; 
	  betasq          <- parms[3];
	  tausq           <- parms[4];
  
	#tt <- Tvec-Tvec[1]
	long.t <- t.i[1]:max(t.i)
	nparms    <- length(parms);
	preds.boot1<- matrix(0,nrow=B,ncol=length(t.i))
	preds.boot2<- matrix(0,nrow=B,ncol=length(long.t))
	
	if(REMLE=="TRUE"){

		boot.remles <- matrix(0,nrow=B,ncol=nparms+1); 
		all.sims    <- ouss_sim(nsims=B,parms=parms,tt=tt);
		all.preds   <- ouss_predict(yt=yt,tt=tt,parms=parms,plot.it="FALSE")
		remle.preds <- all.preds[[1]][,2]
		remle.longpreds <- all.preds[[2]][,2]		

		for(b in 1:B ){
		
			bth.timeseries <- all.sims[,b];
  		remles.out <- ouss_remle(yt=bth.timeseries,tt=tt, fguess = parms);
			boot.remles[b,] <- c(remles.out$remles, remles.out$lnLhat);
			all.bootpreds <- ouss_predict( yt=bth.timeseries, 
			                                  tt=tt, 
			                                  parms=remles.out$remles, 
			                                  plot.it="FALSE");
			preds.boot1[b,] <- all.bootpreds[[1]][,2]
			preds.boot2[b,] <- all.bootpreds[[2]][,2]
			
		} 
	
		CIs.mat <- apply(boot.remles,2,FUN=function(x){
		  quantile(x,probs=c(0.025,0.975))});
		CIs.mat <- rbind(CIs.mat[1,1:4],parms,CIs.mat[2,1:4]);
		rownames(CIs.mat) <- c("Lower_CI_2.5","REMLE","Upper_CI_97.5");
		colnames(CIs.mat) <- c("mu", "theta","betasq","tausq");
		
		preds.CIs1 <- apply(preds.boot1,2,FUN=function(x){quantile(x,probs=c(0.025,0.975))});
		mean.boots <- apply(preds.boot1,2,FUN=function(x){quantile(x,probs=0.50)})
		preds.CIs1 <- t(rbind(tt,
		                      exp(yt),
		                      remle.preds-(mean.boots-preds.CIs1[1,]), 
		                      remle.preds, 
		                      remle.preds+(preds.CIs1[2,]-mean.boots)));
		colnames(preds.CIs1) <- c("Time","Observed","Lower_CI_2.5","REMLE","Upper_CI_97.5");

		preds.CIs2 <- apply(preds.boot2,2,FUN=function(x){quantile(x,probs=c(0.025,0.975))});
		mean.boots2 <- apply(preds.boot2,2,FUN=function(x){quantile(x,probs=0.50)})
		preds.CIs2 <- t(rbind(long.t+tt[1],
		                      remle.longpreds-(mean.boots2-preds.CIs2[1,]), 
		                      remle.longpreds, 
		                      remle.longpreds+(preds.CIs2[2,]-mean.boots2)));

		#preds.CIs2 <- apply(preds.boot2,2,FUN=function(x){quantile(x,probs=c(0.025,0.5,0.975))});
		#preds.CIs2 <- t(rbind(long.t+Tvec[1],preds.CIs2));
		colnames(preds.CIs2) <- c("Time","Lower_CI_2.5","REMLE","Upper_CI_97.5");
		#pred.CIs2 <- cbind(preds.CIs2[,1], reml.longpreds-(preds.CIs2[,3]-preds.CIs2[,2]),reml.longpreds,reml.longpreds+(preds.CIs2[,4]-preds.CIs2[,3]))
		
		boot.list <- list(boot.remles = boot.remles, CIs.mat = CIs.mat, preds.CIs1 = preds.CIs1, 
					 preds.CIs2=preds.CIs2);
		
		if(plot.it=="TRUE"){
			par(mfrow=c(2,2));
			hist(boot.remles[,1],main=expression(hat(mu)),xlab="");
		  abline(v=parms[1],lwd=2,col="red");
			hist(boot.remles[,2],main=expression(hat(theta)),xlab="");
		  abline(v=parms[2],lwd=2,col="red");
			hist(boot.remles[,3],main=expression(hat(beta^2)),xlab="");
		  abline(v=parms[3],lwd=2,col="red");
			hist(boot.remles[,4],main=expression(hat(tau^2)),xlab="");
		  abline(v=parms[4],lwd=2,col="red");
		  par(mfrow=c(1,1));
		}
		return(boot.list)
		
		}else{

		boot.mles <- matrix(0,nrow=B,ncol=nparms+2);
		all.sims  <- ouss_sim(nsims=B, tt=tt, parms=parms);
		all.preds <- ouss_predict(yt=yt, tt=tt, parms=parms, plot.it="FALSE")
		mle.preds<- all.preds[[1]][,2]
		mle.longpreds <- all.preds[[2]][,2]		
		
		for(b in 1:B ){
		
			bth.timeseries <- all.sims[,b];
			mles.out <- ouss_mle(yt = bth.timeseries, tt=tt, fguess = parms);
			boot.mles[b,] <- c(mles.out$mles, mles.out$lnL.hat,mles.out$AIC);
			all.bootpreds <- ouss_predict(yt=bth.timeseries, tt=tt, parms=mles.out$mles, plot.it="FALSE");
			preds.boot1[b,] <- all.bootpreds[[1]][,2]
			preds.boot2[b,] <- all.bootpreds[[2]][,2]
			} 
		
		CIs.mat <- apply(boot.mles,2,FUN=function(x){quantile(x,probs=c(0.025,0.975))});
		CIs.mat <- rbind(CIs.mat[1,1:4],
		                 parms,
		                 CIs.mat[2,1:4]);
		rownames(CIs.mat) <- c("Lower_CI_2.5","MLE","Upper_CI_97.5");
		colnames(CIs.mat) <- c("mu", "theta","betasq","tausq");
		
		#preds.CIs1 <- apply(preds.boot1,2,FUN=function(x){quantile(x,probs=c(0.025,0.975))});
		#preds.CIs1 <- t(rbind(Tvec,preds.CIs1[1,], ml.preds, preds.CIs1[2,]));
		preds.CIs1 <- apply(preds.boot1,2,FUN=function(x){quantile(x,probs=c(0.025,0.5,0.975))});
		preds.CIs1 <- t(rbind(tt,
		                      exp(yt),
		                      preds.CIs1));
		colnames(preds.CIs1) <- c("Time", "Observed","Lower_CI_2.5","MLE","Upper_CI_97.5");
		pred.CIs1 <- cbind(preds.CIs1[,1], 
		                   mle.preds-(preds.CIs1[,3]-preds.CIs1[,2]), 
		                   mle.preds, 
		                   mle.preds+(preds.CIs1[,4]-preds.CIs1[,3]))

		#preds.CIs2 <- apply(preds.boot2,2,FUN=function(x){quantile(x,probs=c(0.025,0.975))});
		#preds.CIs2 <- t(rbind(Tvec,preds.CIs2[1,], ml.preds, preds.CIs2[2,]));
		preds.CIs2 <- apply(preds.boot2,2,FUN=function(x){quantile(x,probs=c(0.025,0.5,0.975))});
		preds.CIs2 <- t(rbind(long.t+tt[1],
		                      preds.CIs2));
		pred.CIs2 <- cbind(preds.CIs2[,1], 
		                   mle.longpreds-(preds.CIs2[,3]-preds.CIs2[,2]),
		                   mle.longpreds,
		                   mle.longpreds+(preds.CIs2[,4]-preds.CIs2[,3]))
		
		colnames(preds.CIs2) <- c("Time","Lower_CI_2.5","MLE","Upper_CI_97.5");
		
		boot.list <- list(boot.mles = boot.mles, 
		                  CIs.mat = CIs.mat, 
		                  preds.CIs1 = preds.CIs1, 
		                  preds.CIs2 = preds.CIs2)

		if(plot.it=="TRUE"){
			par(mfrow=c(2,2));
			hist(boot.mles[,1],main=expression(hat(mu)),xlab="");
		  abline(v=parms[1],lwd=2,col="red");
			hist(boot.mles[,2],main=expression(hat(theta)),xlab="");
		  abline(v=parms[2],lwd=2,col="red");
			hist(boot.mles[,3],main=expression(hat(beta^2)),xlab="");
		  abline(v=parms[3],lwd=2,col="red");
			hist(boot.mles[,4],main=expression(hat(tau^2)),xlab="");
		  abline(v=parms[4],lwd=2,col="red");	
		  par(mfrow=c(1,1));
		}
		
		return(boot.list)
		
	}# End if/else
		
}
```

A little test

```{r}
ouss_pboot(B = 100, yt = yt2, tt = tt2, parms = REMLEstimates.ouss.gap$remles, REMLE = TRUE, plot.it = TRUE)
```

### 3.7.5. A function to run the estimation, compute the predictions and run a parametric bootstrap

With a single function (`ouss_calc()`), we can run estimation, compute predictions, and run parametric bootstrap

```{r}
ouss_calc <- function(yt, tt, pmethod="ML", nboot, plot.pred="TRUE", plot.bootdists = "TRUE"){

	# Compute a rough guess of the parameter estimates to initialize the search:
	guesscalc <- guess_ouss(yt = yt, tt=tt)
	

	# Compute either the ML or the REML estimates, according to what you specified in point 1. above
	if(pmethod=="ML"){
		best.guess <- ouss_mle(yt = yt, tt = tt, fguess = guesscalc);
		AIC <- best.guess[[3]];
		remle.option <- "FALSE"}else if (pmethod=="REML"){
			best.guess <- ouss_remle(yt = yt, tt = tt, fguess = guesscalc);
			remle.option <- "TRUE"}else{
				print("Error: ML and REML are the only options allowed for 'method'")}

	# Parameter estimates and maximized log-likelihood (we will print these at the end)
	parms.est <- best.guess[[1]];
	lnLhat    <- best.guess[[2]];

	# Parametric bootstrap: computing both, parameters and predictions 95 % CI's
	pboot.calcs <- ouss_pboot(B=nboot, 
	                             yt = yt, 
	                             tt = tt, 
	                             REMLE = remle.option, 
	                             parms = parms.est, 
	                             plot.it = plot.bootdists);
	
	if(plot.pred=="TRUE"){
	  plot(pboot.calcs$preds.CIs1[,1],
	       pboot.calcs$preds.CIs1[,2], 
	       xlab = "Time", ylab = "Population abundance", 
	       type = "b", col = "blue", cex = 1.5);
		points(pboot.calcs$preds.CIs2[,1],
		       pboot.calcs$preds.CIs2[,3],type="l",col="red", lty = "dashed");
		points(tt,pboot.calcs$preds.CIs1[,4],type="p",col="red");
	  points(pboot.calcs$preds.CIs2[,1],
		       pboot.calcs$preds.CIs2[,2],type="l",col="gray", lty = "dotted");
	  	  points(pboot.calcs$preds.CIs2[,1],
		       pboot.calcs$preds.CIs2[,4],type="l",col="gray", lty = "dotted");
	  legend("top", legend=c("Observed", "Predicted", "CI"),
       col=c("blue","red", "gray"), lty=1:3, cex=0.8)
		}
	
	if(pmethod=="ML"){
		print("AIC score");
	  print(AIC);
		out <- list(parms.est = parms.est, 
		            lnLhat = lnLhat, 
		            AIC = AIC, 
		            pbootmat = pboot.calcs[[1]], 
		            pboot.cis = pboot.calcs[[2]], 
		            pboot.preds1 = pboot.calcs$preds.CIs1, 
		            pboot.preds2 = pboot.calcs$preds.CIs2)}else{
			out <- list(parms.est = parms.est, 
			            lnLhat = lnLhat, 
			            pbootmat = pboot.calcs[[1]], 
			            pboot.cis = pboot.calcs[[2]], 
			            pboot.preds1 = pboot.calcs$preds.CIs1,
			            pboot.preds2 = pboot.calcs$preds.CIs2)}
	
	return(out);
}
```


Now we can fill the parameters and test our estimates. First with the dataset of complete values:

```{r}
AmericanRedstarNOGaps <- ouss_calc(yt = yt1, 
                                 tt = tt1, 
                                 pmethod = "REML", 
                                 nboot = 100, 
                                 plot.pred = TRUE, 
                                 plot.bootdists = T)
```

And now with the dataset with gaps
```{r}
AmericanRedstarGaps <- ouss_calc(yt = yt2, 
                                 tt = tt2, 
                                 pmethod = "REML", 
                                 nboot = 100, 
                                 plot.pred = TRUE, 
                                 plot.bootdists = T)
```

Although the confidence intervals increase, the general pattern is recovered!


## Population Viability Monitoring or Risk-based viable population monitoring

Let use our time-series example to estimate the probability of extinction in a risk-based viable population monitoring. The aim is to find a temporal trend of extinction risk. Our example on American Redstart monitoring the abundance of the population for 30 years. We arbitrarily removed 6, to have gaps in the time-series. We want to assess if the extinction risk of the population has been increasing or decreasing in the last 20 years. 

First, let fit a stochastic population dynamic model for the first 10 observation points, storing the estimates of the parameters:

```{r}
last.tt <- 10

yt2[1:last.tt] #It was already converted to the log-abundance
tt2[1:last.tt]

OUSS.partial <- ouss_remle(yt = yt2[1:last.tt], 
                           tt = tt2[1:last.tt],
                           fguess = guess_ouss(yt = yt2[1:last.tt],
                                               tt = tt2[1:last.tt]))

#Estimated parameters
OUSS.partial$remles

#is theta < 0.025?; If TRUE, we should use EGSS to fit and predict!
  #If FALSE, we can use OUSS
model <- if(OUSS.partial$remles[2] < 0.025){"EGSS"}else{"OUSS"}
```

With the estimated model parameters and the data up to the first 10 years, the probability that the population will crash below a critical threshold ($N_{critical}$, as the critical number of individuals) is estimated for the next 5 years. Let assume this $N_{critical} = 3/4 (\exp\bar{(y)})$. The resulting probability is recorded.

```{r}
#Define N_critical
N.critical = (3/4)*mean(exp(yt2))

#number of simulations
ntraj = 100

#threshold times
thres.times <- 0:5

#Maximum simulation length
len.sim <- max(thres.times)+1

#last observed time
m = last(tt2[1:last.tt])

#plot the abundance data
plot(tt2[1:last.tt], 
     exp(yt2[1:last.tt]), 
     type="b", lwd=2, cex.lab=1.25, 
     xlim=c(min(tt2),max(tt2)),
     ylim=c(0,30), 
     ylab="Population abundance", xlab="Time")

#initiating array to store last points of simulation at time m+5
last.points <- rep(0,ntraj)

#simulate population 100 trends 
for(j in 1:ntraj){
  sim.mat <- ouss_sim(1,
                      tt = thres.times,
                      parms = OUSS.partial$remles)
  Pop.sim <- exp(c(yt2[last.tt], sim.mat[-1]))
  
  if(Pop.sim[len.sim]>=N.critical){
    lines(m+thres.times, Pop.sim, col="lightgrey", lty = "solid")
  }else{lines(m+thres.times, Pop.sim, col="red", lty = "solid")}
  
  last.points[j] <- Pop.sim[len.sim]
}

#add critical value line
abline(h=N.critical, lty=2, lwd=1)	

#How many of the simulations (ntraj) end below N_critical? Probability of extinction
P.ext <- length(which(round(last.points,2) <= N.critical))/ntraj

#Lognormal with library(ke31d)

kde.sims <- kde1d(x=last.points)
#plot(kde.sims)

prob.ext <- pkde1d(q=N.critical, obj=kde.sims)

lines(x = (kde.sims$values*25 + m + 5),
        y = kde.sims$grid_points,
      col="darkgray", lwd=2, lty=1)

lines(x = (kde.sims$values[kde.sims$grid_points <= N.critical]*25 + m + 5),
        y = kde.sims$grid_points[kde.sims$grid_points <= N.critical],
      col="red", lwd=2, lty=1)

title(main=paste0(model, " - P(ext risk) = ", signif(mean(c(prob.ext, P.ext)), digits=2)), cex=1.5)

```

Now, we can add the next observation (11) to the time-series, and the model parameters are re-estimated as well as the probability of crashing below the $N_{critical}$ for the next 5 years, and so on, so on. Let presents 4 moments


```{r}
last.tt <- 11

yt2[1:last.tt] #It was already converted to the log-abundance
tt2[1:last.tt]

OUSS.partial <- ouss_remle(yt = yt2[1:last.tt],
                           tt = tt2[1:last.tt],
                           fguess = guess_ouss(yt = yt2[1:last.tt],
                                               tt = tt2[1:last.tt]))

OUSS.partial$remles 

model <- if(OUSS.partial$remles[2] < 0.025){"EGSS"}else{"OUSS"}

EGSS.partial <- egss_mle(yt = yt2[1:last.tt], 
                         tt = tt2[1:last.tt],
                         fguess_egss = guess_egss(yt = yt2[1:last.tt],
                                                  tt = tt2[1:last.tt]))

N.critical = (3/4)*mean(exp(yt2))

ntraj = 100
thres.times <- 0:5
len.sim <- max(thres.times)+1
m = last(tt2[1:last.tt])
plot(tt2[1:last.tt], 
     exp(yt2[1:last.tt]), 
     type="b", lwd=2, cex.lab=1.25, 
     xlim=c(min(tt2),max(tt2)),
     ylim=c(0,30), 
     ylab="Population abundance", xlab="Time")
last.points <- rep(0,ntraj)
for(j in 1:ntraj){
  sim.mat <- egss_sim(1,
                      tt = thres.times,
                      parms = EGSS.partial$mles)
   Pop.sim <- exp(c(yt2[last.tt], sim.mat[-1]))
 
  if(Pop.sim[len.sim]>=N.critical){
    lines(m+thres.times, Pop.sim, col="lightgrey", lty = "solid")
  }else{lines(m+thres.times, Pop.sim, col="red", lty = "solid")}
  
  last.points[j] <- Pop.sim[len.sim]
}

abline(h=N.critical, lty=2, lwd=1)	

#How many of the simulations (ntraj) end below N_critical? Probability of extinction
probability.ext1 <- length(which(round(last.points,2) <= N.critical))/ntraj

#Lognormal with library(ke31d)

kde.sims <- kde1d(x=last.points)
#plot(kde.sims)

probability.ext2 <- pkde1d(q=N.critical, obj=kde.sims)

probability.ext <- mean(c(probability.ext1, probability.ext2))

lines(x = (kde.sims$values*25 + m + 5),
        y = kde.sims$grid_points,
      col="darkgray", lwd=2, lty=1)

lines(x = (kde.sims$values[kde.sims$grid_points <= N.critical]*25 + m + 5),
        y = kde.sims$grid_points[kde.sims$grid_points <= N.critical],
      col="red", lwd=2, lty=1)

title(main=paste0(model, " - P(ext risk) = ", signif(probability.ext, digits=2)), cex=1.5)

```


Iterating this process, for instance for years position 10 to 24:

```{r}
#A vector of time steps to evaluate
last.tt <- c(10:24)

N.critical = (1/4)*mean(exp(yt2))
ntraj = 100
thres.times <- 0:5
len.sim <- max(thres.times)+1

#array to store p.below critical point (last point < N.critical/ntraj)
  probability.ext1 <- rep(0,length(last.tt))
#array to store p.below critical point - area under the curve
  probability.ext2 <- rep(0,length(last.tt))

par(mfrow=c(5,3),mar=c(3,3,3,1),  oma=c(2,2,2,1), mgp=c(2,0.5,0))

for(t in last.tt){
  plot(tt2[1:t], 
     exp(yt2[1:t]), 
     type="b", lwd=2, cex.lab=1.25, 
     xlim=c(min(tt2),max(tt2)),
     ylim=c(0,30), 
     ylab="Population abundance", xlab="Time")
  
  last.points <- rep(0,ntraj)

  OUSS.partial <- ouss_remle(yt = yt2[1:t],
                           tt = tt2[1:t],
                           fguess = guess_ouss(yt = yt2[1:t],
                                               tt = tt2[1:t]))

  model <- if(OUSS.partial$remles[2] < 0.025){
    "EGSS"
    }else{
      "OUSS"
      }
  
    if(model == "OUSS"){
      
      for(j in 1:ntraj){
      
      sim.mat <- ouss_sim(1,
                      tt = thres.times,
                      parms = OUSS.partial$remles)
      
      Pop.sim <- exp(c(yt2[t], sim.mat[-1]))
  
        if(Pop.sim[len.sim]>=N.critical){
        
          lines(tt2[t]+thres.times, Pop.sim, col="lightgrey", lty = "solid")
        }else{
          lines(tt2[t]+thres.times, Pop.sim, col="red", lty = "solid")}
      
      last.points[j] <- Pop.sim[len.sim]
      }
      
      abline(h=N.critical, lty=2, lwd=1)	
      
      probability.ext1[t] <- length(which(round(last.points,2) <= N.critical))/ntraj
      
      kde.sims <- kde1d(x=last.points)
      probability.ext2[t] <- pkde1d(q=N.critical, obj=kde.sims)
      
      probability.ext[t] <- mean(c(probability.ext1[t], probability.ext2[t]))
      
      lines(x = (kde.sims$values*10 + tt2[t] + 5),
            y = kde.sims$grid_points,
            col="darkgray", lwd=2, lty=1)
      
      lines(x = (kde.sims$values[kde.sims$grid_points <= N.critical]*10 + tt2[t] + 5),
            y = kde.sims$grid_points[kde.sims$grid_points <= N.critical],
            col="red", lwd=2, lty=1)
      
      title(main=paste0(tt2[t], "-", model, "- P(ext risk) = ", signif(probability.ext[t], digits=2)), cex=1.5)}
  
  else{for(j in 1:ntraj){
    
    EGSS.partial <- egss_mle(yt = yt2[1:t],
                           tt = tt2[1:t],
                           fguess_egss = guess_egss(yt = yt2[1:t],
                                               tt = tt2[1:t]))
    
    sim.mat <- egss_sim(1,
                      tt = thres.times,
                      parms = EGSS.partial$mles)
    Pop.sim <- exp(c(yt2[t], sim.mat[-1]))
  
      if(Pop.sim[len.sim]>=N.critical){
        
        lines(tt2[t]+thres.times, Pop.sim, col="lightgrey", lty = "solid")
      }else{
          lines(tt2[t]+thres.times, Pop.sim, col="red", lty = "solid")}
      
      last.points[j] <- Pop.sim[len.sim]
      }
      
      abline(h=N.critical, lty=2, lwd=1)	
      
      probability.ext1[t] <- length(which(round(last.points,2) <= N.critical))/ntraj
      
      kde.sims <- kde1d(x=last.points)
      probability.ext2[t] <- pkde1d(q=N.critical, obj=kde.sims)
      
      probability.ext[t] <- mean(c(probability.ext1[t], probability.ext2[t]))
      
      lines(x = (kde.sims$values*10 + tt2[t] + 5),
            y = kde.sims$grid_points,
            col="darkgray", lwd=2, lty=1)
      
      lines(x = (kde.sims$values[kde.sims$grid_points <= N.critical]*10 + tt2[t] + 5),
            y = kde.sims$grid_points[kde.sims$grid_points <= N.critical],
            col="red", lwd=2, lty=1)
      
      title(main=paste0(tt2[t], "-", model, "- P(ext risk) = ", signif(probability.ext[t], digits=2)), cex=1.5)}

}

par(mfrow=c(1,1))

plot(tt2[-1],probability.ext[-1], type = "b", ylim = c(0,1), ylab = "Probability of extinction risk", xlab = "Time (year)")
```

How different is this "gap" population from the complete dataset?

```{r}
#A vector of time steps to evaluate
last.tt <- c(10:29)

N.critical = (1/4)*mean(exp(yt1))
ntraj = 100
thres.times <- 0:5
len.sim <- max(thres.times)+1

#array to store p.below critical point (last point < N.critical/ntraj)
  probability.ext1 <- rep(0,length(last.tt))
#array to store p.below critical point - area under the curve
  probability.ext2 <- rep(0,length(last.tt))

par(mfrow=c(4,5),mar=c(3,3,3,1),  oma=c(2,2,2,1), mgp=c(2,0.5,0))

for(t in last.tt){
  plot(tt1[1:t], 
     exp(yt1[1:t]), 
     type="b", lwd=2, cex.lab=1.25, 
     xlim=c(min(tt1),max(tt1)),
     ylim=c(0,30), 
     ylab="Population abundance", xlab="Time")
  
  last.points <- rep(0,ntraj)

  OUSS.partial <- ouss_remle(yt = yt1[1:t],
                           tt = tt1[1:t],
                           fguess = guess_ouss(yt = yt1[1:t],
                                               tt = tt1[1:t]))

  model <- if(OUSS.partial$remles[2] < 0.025){
    "EGSS"
    }else{
      "OUSS"
      }
  
    if(model == "OUSS"){
      
      for(j in 1:ntraj){
      
      sim.mat <- ouss_sim(1,
                      tt = thres.times,
                      parms = OUSS.partial$remles)
      
      Pop.sim <- exp(c(yt1[t], sim.mat[-1]))
  
        if(Pop.sim[len.sim]>=N.critical){
        
          lines(tt1[t]+thres.times, Pop.sim, col="lightgrey", lty = "solid")
        }else{
          lines(tt1[t]+thres.times, Pop.sim, col="red", lty = "solid")}
      
      last.points[j] <- Pop.sim[len.sim]
      }
      
      abline(h=N.critical, lty=2, lwd=1)	
      
      probability.ext1[t] <- length(which(round(last.points,2) <= N.critical))/ntraj
      
      kde.sims <- kde1d(x=last.points)
      probability.ext2[t] <- pkde1d(q=N.critical, obj=kde.sims)
      
      probability.ext[t] <- mean(c(probability.ext1[t], probability.ext2[t]))
      
      lines(x = (kde.sims$values*10 + tt1[t] + 5),
            y = kde.sims$grid_points,
            col="darkgray", lwd=2, lty=1)
      
      lines(x = (kde.sims$values[kde.sims$grid_points <= N.critical]*10 + tt2[t] + 5),
            y = kde.sims$grid_points[kde.sims$grid_points <= N.critical],
            col="red", lwd=2, lty=1)
      
      title(main=paste0(tt1[t], "-", model, "- P(ext risk) = ", signif(probability.ext[t], digits=2)), cex=1.5)}
  
  else{for(j in 1:ntraj){
    
    EGSS.partial <- egss_mle(yt = yt1[1:t],
                           tt = tt1[1:t],
                           fguess_egss = guess_egss(yt = yt1[1:t],
                                               tt = tt1[1:t]))
    
    sim.mat <- egss_sim(1,
                      tt = thres.times,
                      parms = EGSS.partial$mles)
    Pop.sim <- exp(c(yt1[t], sim.mat[-1]))
  
      if(Pop.sim[len.sim]>=N.critical){
        
        lines(tt1[t]+thres.times, Pop.sim, col="lightgrey", lty = "solid")
      }else{
          lines(tt1[t]+thres.times, Pop.sim, col="red", lty = "solid")}
      
      last.points[j] <- Pop.sim[len.sim]
      }
      
      abline(h=N.critical, lty=2, lwd=1)	
      
      probability.ext1[t] <- length(which(round(last.points,2) <= N.critical))/ntraj
      
      kde.sims <- kde1d(x=last.points)
      probability.ext2[t] <- pkde1d(q=N.critical, obj=kde.sims)
      
      probability.ext[t] <- mean(c(probability.ext1[t], probability.ext2[t]))
      
      lines(x = (kde.sims$values*10 + tt1[t] + 5),
            y = kde.sims$grid_points,
            col="darkgray", lwd=2, lty=1)
      
      lines(x = (kde.sims$values[kde.sims$grid_points <= N.critical]*10 + tt1[t] + 5),
            y = kde.sims$grid_points[kde.sims$grid_points <= N.critical],
            col="red", lwd=2, lty=1)
      
      title(main=paste0(tt1[t], "-", model, "- P(ext risk) = ", signif(probability.ext[t], digits=2)), cex=1.5)}

}

par(mfrow=c(1,1))

plot(tt1,probability.ext, type = "b", ylim = c(0,1), ylab = "Probability of extinction risk", xlab = "Time (year)")
```



Thus, we can have a function to conduct this PVM (or vpm), requiring:
1. A vector of log-abundance observations in the time-series ($y_t$)
2. A vector of observations times ($t_t$)
3. The number of trajectories (e.g., 100, 1000) to simulate (`ntraj`)
4. Critical value of the population (quasiextinction criteria; e.g., $\frac{1}{4}\bar{y}$)
5. Population long-term simulations (for how long the population will be simulated?)

```{r}
vpm <- function(yt, tt, ntraj, N.critical, plt){
  # A vector of time steps to evaluate
    last.tt <- c(round(1/4*length(tt),0):length(tt))
  # Number of trajectories to simulate
    ntraj = ntraj
  # threshold time steps and length
    thres.times <- 0:plt
    len.sim <- max(thres.times)+1
  # Confirming decimals in N.critical
    N.critical = round(N.critical,3)
  #array to store p.below critical point (last point < N.critical/ntraj)
    probability.ext1 <- rep(0,length(last.tt))
  #array to store p.below critical point - area under the curve
    probability.ext2 <- rep(0,length(last.tt))
  #array to store model in each time
    Model <- rep(0,length(last.tt))

  for(t in last.tt){
    #array to store last point of each trajectory
      last.points <- rep(0,ntraj)
    #Compute Restricted Maximum Likelihood Estimaton for OUSS
      OUSS.partial <- ouss_remle(yt = yt[1:t],
                             tt = tt[1:t],
                             fguess = guess_ouss(yt = yt[1:t],
                                                 tt = tt[1:t]))
    #Select the model to simulate based on density dependence parameter
      model <- if(OUSS.partial$remles[2] < 0.025){"EGSS"}else{"OUSS"}
      
      Model[t] <- model;

    if(model == "OUSS"){
      for(j in 1:ntraj){
      #simulate the next PLT values (5, 10, 20,...)
        sim.mat <- ouss_sim(1,
                          tt = thres.times,
                          parms = OUSS.partial$remles)

        Pop.sim <- exp(c(yt[t], sim.mat[-1]))
        last.points[j] <- Pop.sim[len.sim]
      }

      probability.extinction1 <- length(which(round(last.points,3) <= N.critical))/ntraj

      probability.ext1[t] <- if(probability.extinction1 <= 0.001){"≤0.001"}else{probability.extinction1}

      kde.sims <- kde1d(x=last.points)
      probability.extinction2 <- round(pkde1d(q=N.critical, obj=kde.sims),3)
      
      probability.ext2[t] <- if(probability.extinction2 <= 0.001){"≤0.001"}else{probability.extinction2}
      }
    else{
      for(j in 1:ntraj){
      #Compute Maximum Likelihood Estimators for EGSS
        EGSS.partial <- egss_mle(yt = yt[1:t],
                             tt = tt[1:t],
                             fguess_egss = guess_egss(yt = yt[1:t],
                                                      tt = tt[1:t]))
      #simulate the next PLT values (5, 10, 20,...)
        sim.mat <- egss_sim(1,
                        tt = thres.times,
                        parms = EGSS.partial$mles)
        Pop.sim <- exp(c(yt[t], sim.mat[-1]))
        last.points[j] <- Pop.sim[len.sim]
      }

      probability.extinction1 <- length(which(round(last.points,3) <= N.critical))/ntraj

      probability.ext1[t] <- if(probability.extinction1 <= 0.001){"≤0.001"}else{probability.extinction1}

      kde.sims <- kde1d(x=last.points)
      probability.extinction2 <- round(pkde1d(q=N.critical, obj=kde.sims),3)
      
      probability.ext2[t] <- if(probability.extinction2 <= 0.001){"≤0.001"}else{probability.extinction2}
    }
      }
  
    out <- tibble(Time = tt,
                  Model = Model,
                  Prob.ext.Risk.1 = probability.ext1,
                  Prob.ext.Risk.2 = probability.ext2,
                  Observed = exp(yt))
    return(out)
    }
```


```{r}
Setoruti.vpm <- vpm(yt = yt1, tt = tt1, ntraj = 100, N.critical = 3/4*mean(exp(yt1)), plt = 10);

ggplot(Setoruti.vpm, aes(x = Time))+
  geom_line(aes(y = Prob.ext.Risk.1), color = "darkred", alpha = 0.5)+
  geom_line(aes(y = Prob.ext.Risk.2), color = "darkgreen", alpha = 0.5)+
  geom_point(aes(y = Prob.ext.Risk.1, shape = Model), color = "darkred", alpha = 0.5)+
  geom_point(aes(y = Prob.ext.Risk.2, shape = Model), color = "darkgreen", alpha = 0.5)+
  labs(x = "Time (years)",
       y = "Probability of extinction risk")+
  ylim(0:1)+
  annotate("text", x = c(1985, 1985), y = c(0.42,0.24), 
           label = c("Proportion simulations","density lognormal"),
           color = c("darkred","darkgreen"))+
  theme_classic()+
  theme(legend.position = "inside",
        legend.position.inside = c(0.8, 0.7))
```

# Prepare the data from eBird and time-series vectors

We can simplify the eBird data selecting only columns of our interest (it will reduce the size of the dataset)

```{r eval=F}
colsE <- c("observer_id", "sampling_event_identifier",
           "group identifier",
           "common_name", "scientific_name",
           "observation_count",
           "country", "state_code", "locality_id", "latitude", "longitude",
           "protocol_type", "all_species_reported",
           "observation_date",
           "time_observations_started",
           "duration_minutes", "effort_distance_km",
           "number_observers")
```

To conduct some filters, we will generate temporal files in our computer. Here we generate only a single temporal file, overwriting for the different species.

```{r eval=F}
#generate a temporal file to save the filtering eBird data (f_ebd) and sampling (f_sed)
f_ebd <- "data_tmp/ebd_Examples.txt" 
f_sed <- "data_tmp/sed_Examples.txt" 
```

The construction of time-series of the estimation of individuals from eBird will assume spatiotemporal subsampling for the high counts (assuming to be the minimum number of individuals) per day in spatial sampling units of $100 \text{ km}^2$.

```{r eval=F}
#Spatial grid cells - diameters of ~11km (area of 95.98 ~> ~100 km^2)
set.seed(123)
dggs_pop <- dgconstruct(spacing = 11) 
#spacing 11 correspond to Characteristic Length Scale (CLS), or diameter of spherical cell
```

Now, we can download the data from [eBird](https://ebird.org/data/download). Inexplicable, although we select for include the sampling event data for other examples (previous approach), only the Snail Kite in Florida contain such information. Nonetheless, we will use only the detection and count estimation by the observers. We save our eBird data files in the folder `Abundance_data`.

### Snail Kite in Florida, US

Filtering for protocol, distance, duration, only complete lists

```{r eval=F}
ebd_filt <- auk_ebd("data_raw/ebd_US-FL_snakit_smp_relMay-2024.txt") %>%
  auk_protocol(c("Traveling", "Stationary")) %>%
  auk_distance(distance = c(0,5)) %>%
  auk_duration(duration = c(0,300))%>%
  auk_complete() %>%
  auk_filter(f_ebd,overwrite=T, keep = colsE) %>%
  read_ebd()
```

Then, just for the sake of double check and organization, we can remove the observations without count estimation, add distance $0$ to stationary protocols, modify the time of observations started to decimal and round hour sampling to an integer, extract year, month, week, and day_of_year. Also, we can confirm and filter out by effort, such as observers $≤10$, distance $≤5 \text{ km}$, duration $≤5 \text{ hours}$, and only records with abundance estimation.

```{r eval=F}
#Some effort extraction and confirmation
ebd_filt <- ebd_filt %>%
  mutate(
    # I don't want here count in 'X', to convert to NA
    observation_count = as.integer(observation_count),
    # effort_distance_km to 0 for non-travelling counts
    effort_distance_km = if_else(protocol_type == "Stationary",
                                 0, effort_distance_km),
    # convert time to decimal hours since midnight
    time_observations_started = time_to_decimal(time_observations_started),
    hour_sampling = round(time_observations_started, 0),
    # split date into year, month, week, and day of year
    year = year(observation_date),
    month = month(observation_date),
    week = week(observation_date),
    day_of_year = yday(observation_date)) %>%
  filter(number_observers <= 10,         #Only list with less than 10 observers
         effort_distance_km <= 5,        #be sure of distance effort
         duration_minutes %in% (0:300),  #be sure of duration effort
         !is.na(observation_count))      #only records with abundance estimation
```

Now we can add a new variable that identify each cell from a grid of hexagons (spatial sampling units).

```{r eval=F}
#add a new variable that identify cell, and have the maximum count per week per cell
SnailKite <- ebd_filt %>%
  mutate(cell = dgGEO_to_SEQNUM(dggs_pop, #id for cells
                                longitude, latitude)$seqnum) %>%
  group_by(cell, year, month, week) %>%
  mutate(max_count = max(observation_count, na.rm = T), 
         n_lists = n()) |>
  ungroup()

```

And we adjust time-series from the 1st week of 2018 (January) to the 22nd of 2024 (May). Since 2018, Snail kites reached more than 1000 records (actually, more than 800). In addition, in 2018 was the species established in Paynes' Prairie. We extract the high count per week

```{r eval=FALSE}
hist(SnailKite$year, breaks = 50, main = "Florida Snail kites, checklists per year", xlab = "Year")
abline(h = 1000, v = 2017, col = "red")

snailkites.week <- SnailKite |>
  filter(observation_date >= "2018-01-01") |>
  mutate(Time.t = case_when(year == 2018 ~ week,
                            year > 2018 ~ week+(52*(year-2018)))) #|> View()
  
snailkites.week.counts <- snailkites.week |>
  group_by(cell, Time.t) |>
  summarise(Observed.y = round(max(max_count),0))

```

And we can save this as an outcome for backup.

```{r eval=F}
saveRDS(SnailKite, "data_tmp/SnailKiteCellsID_filtered.rds")
saveRDS(snailkites.week, "data_tmp/SnailKiteCellsWeek.rds")
saveRDS(snailkites.week.counts, "data_tmp/SnailKiteCellsCountsWeek.rds")
```

This file will serve to generate a map figure with the sampling effort after filtering the eBird data following best practices (see Johnston *et al.*, 2021).

### Snail Kite in Payne's Prairie, Alachua County, FL

Now we focused on the sampling unit with higher number of checklists, which correspond to the Payne's Prairie State Park wetland system in Alachua County. For this locality, we also have counts published from Poli *et.al.* (2020), and 2 areas that are surveyed: Payne's Prairie and Payne's Prairie Central.

![Two areas surveyed by Snail Kite project](data_raw/PP and PPC map-min.png)
This wetland overlap with an hexagonal cell that concentrate most records.

#### Figure 1 - Map of sampling units
```{r}
#A global map to make figures ###
world1 <- sf::st_as_sf(maps::map(database = 'world', plot = FALSE, fill = TRUE))
world1

#Get the number of observations in each cell
CellObservationsSK   <- SnailKite %>% 
  group_by(cell) %>%
  summarise(count=n())

#Get the grid cell boundaries for cells which had quakes
gridSnailKite <- dgcellstogrid(dggs_pop,CellObservationsSK$cell)

#Update the grid cells' properties to include the number of lists in each cell
gridSnailKite <- merge(gridSnailKite, CellObservationsSK, by.x="seqnum", by.y="cell")

# Handle cells that cross 180 degrees
wrapped_gridSnailKite = st_wrap_dateline(gridSnailKite, 
                                         options = c("WRAPDATELINE=YES","DATELINEOFFSET=180"), quiet = TRUE)

my_breaks = c(5, 50, 500, 5000)

arrow1 <- tibble(
  x1 = -82.3,
  x2 = -80.75,
  y1 = 29.6,
  y2 = 29.7
)

Fig1a <- ggplot() +
  geom_sf(data = world1)+
  geom_sf(data=wrapped_gridSnailKite,
          aes(color = count, 
              fill = count),
          alpha = 0.7) +
  #  geom_point(data = SnailKite, aes(x = longitude, y = latitude), size = 0.1)+
  scale_color_gradient(low="#440154", 
                       high="#FDE725",
                       trans = "log10",  
                       breaks = my_breaks,
                       labels = my_breaks)+
  scale_fill_gradient(low="#440154", 
                      high="#FDE725",
                      trans = "log10",  
                      breaks = my_breaks,
                      labels = my_breaks)+
  coord_sf(xlim = c(-84.5, -79.5), 
           ylim =  c(24.1, 30.9)) +
  labs(y = "Latitude",
       x = "Longitude",       
       tag = expression(bold("(a)")),
       title = "Snail Kites in Florida",
       subtitle = "eBird effort in spatial sampling units",
       color = expression(Log["10"]~"lists"),
       fill = expression(Log["10"]~"lists")) +
  annotate("text", x = -80, y = 29.75, label = "Payne's \n Prairie") +
  geom_curve(data = arrow1, aes(x = x1, y = y1, xend = x2, yend = y2),
             arrow = arrow(length = unit(0.08, "inch")), size = 0.5,
             color = "red", curvature = -0.3) +
  theme_classic()+
  theme(legend.position = c(0.2,0.25),
        legend.direction = "vertical",
        legend.box.background = element_rect(colour = "black"))

#Zoom to Payne's Prairie
arrow2 <- tibble(
  x1 = c(-82.328576, -82.334182, -82.303112, -82.292372),
  x2 = c(-82.3, -82.375, -82.25, -82.2),
  y1 = c(29.619306, 29.574222, 29.606876, 29.549109),
  y2 = c(29.7, 29.475, 29.65, 29.535)
)

Fig1b <- ggplot() +
  geom_sf(data=wrapped_gridSnailKite,
          aes(color = count, 
              fill = count)) +
  geom_point(data = SnailKite, aes(x = longitude, y = latitude), 
             size = 0.5, alpha = 0.25)+
  scale_color_gradient(low="#440154", 
                       high="#FDE725",
                       trans = "log10",  
                       breaks = my_breaks,
                       labels = my_breaks)+
  scale_fill_gradient(low = alpha("#440154", 0.25), 
                      high = alpha("#FDE725", 0.25),
                      trans = "log10",  
                      breaks = my_breaks,
                      labels = my_breaks)+
  coord_sf(xlim = c(-82.475, -82.125), 
           ylim =  c(29.45, 29.725),
           expand = T) +
  labs(y = "Latitude",
       x = "Longitude",       
       tag = expression(bold("(b)")),
       title = "Snail Kites in Payne's Prairie wetland",
       subtitle = "with eBird records and popular localities") +
  annotate("text", x = -82.25, y = 29.71, label = "Sweetwater Wetlands \n Park") +
  annotate("text", x = -82.4, y = 29.475, label = "US-441") +
  annotate("text", x = -82.2, y = 29.65, label = "La Chua trail") +
  annotate("text", x = -82.2, y = 29.525, label = "Wacahoota trail") +
  geom_curve(data = arrow2, aes(x = x1, y = y1, xend = x2, yend = y2),
             arrow = arrow(length = unit(0.08, "inch")), size = 0.5,
             color = "red", curvature = -0.3) +
  theme_classic()+
  theme(legend.position = "none")

Fig1 <- grid.arrange(Fig1a, Fig1b, ncol = 2, widths = c(1, 2))

ggsave("results/Fig1_SnailKitesMap.pdf", 
       plot = Fig1, dpi = 300, width = 10, height = 5, units = "in")

```

#### Time series of Payne's Prairie

```{r}
#Saved data
SnailKite <- readRDS("data_tmp/SnailKiteCellsID_filtered.rds")
snailkites.week <- readRDS("data_tmp/SnailKiteCellsWeek.rds")
snailkites.week.counts <- readRDS("data_tmp/SnailKiteCellsCountsWeek.rds")

#Cell with more values
CellTop <- SnailKite |>
  group_by(cell) |>
  mutate(n_checklists = n()) |>
  ungroup() |>
  filter(n_checklists == max(n_checklists)) |>
  select(cell) |>
  unique()

#The monitoring data by Poli et al. (2020)
Poli.etal <- data.frame(observation.date = c("2018-02-19",
                                             "2018-03-11",
                                             "2018-04-08",
                                             "2018-05-13",
                                             "2018-05-31",
                                             "2018-06-08",
                                             "2018-07-15",
                                             "2018-08-07",
                                             "2018-08-22",
                                             "2018-09-01",
                                             "2018-10-15",
                                             "2018-12-16"),
                        abundance.monitored = c(1,2,4,1,6,8,6,6,12,9,7,29))

Poli.etal <- Poli.etal |>
  mutate(observation.date = ymd(observation.date),
         year = year(observation.date),
         week = week(observation.date),
         Time.t = case_when(year == 2018 ~ week,
                            year > 2018 ~ week+(52*(year-2018))))

#The monitoring data by standardized methods 2019
snail.kite.project.pp <- read_csv("data_raw/Snail kite surveys on PP and PPC 2018_2024.csv") |> 
  mutate(observation.date = mdy(date),
         year = year(observation.date),
         week = week(observation.date),
         Time.t = case_when(year == 2018 ~ week,
                            year > 2018 ~ week+(52*(year-2018)))) |>
  group_by(observation.date, Time.t, year, week) |>
  summarise(abundance.monitored = max(count))

#including Poli et al.
snail.kite.project.pp <- snail.kite.project.pp |>
  full_join(Poli.etal)

#Generate the time-series for the cell with more records
snailkites.paynesp <- snailkites.week.counts |>
  filter(cell == CellTop$cell)

#To return dates from original data
datesPP <- snailkites.week |> 
  group_by(Time.t) |> 
  summarise(observation.date = min(observation_date))

snailkites.paynesp <- snailkites.paynesp |>
  left_join(datesPP)

#Add standardized monitored
snailkites.PP <- snailkites.paynesp |>
  full_join(snail.kite.project.pp)

#Observation
snailkites.PP |>
  pivot_longer(cols = !c(Time.t, cell, year, week, observation.date),
               names_to = "group",
               values_to = "Abundance") |> 
  drop_na(Abundance) |>
  ggplot(aes(x = observation.date, y = Abundance, fill = group))+
    geom_line(aes(color = group)) +
    geom_point(aes(shape = group), color = "black")+
    labs(x = "Observation date",
       y = "Population abundance",
       tag = "",
       fill = "",
       color = "",
       shape = "")+
    scale_color_manual(values = c("black", "blue"),
                       labels = c("Standardized Monitored","eBird observations"))+
    scale_fill_manual(values = c("black", "blue"),
                       labels = c("Standardized Monitored","eBird observations"))+
    scale_shape_manual(values = c(19, 21),
                       labels = c("Standardized Monitored","eBird observations"))+
    theme_classic()+
    theme(legend.position = c(0.2, 0.8))
```

We will fit an EGSS model for the first two years (weeks from 1 to 104)

```{r}
snailkites.paynesp.init <- snailkites.PP |>
  filter(Time.t %in% c(1:104))

```

Lets fit a first EGSS model for the eBird data observations
```{r}
#Define variables
yt1 = snailkites.paynesp.init |>
  ungroup() |>
  arrange(Time.t) |>
  drop_na(Observed.y) |>
  select(Observed.y) |>
  log()

tt1 <- snailkites.paynesp.init |>
  ungroup() |>
  arrange(Time.t) |>
  drop_na(Observed.y) |>
  select(Time.t)


sk.egss.parms <- egss_mle(yt = yt1$Observed.y,
                          tt = tt1$Time.t,
                          fguess_egss = guess_egss(yt = yt1$Observed.y,
                                                   tt = tt1$Time.t))

sk.egss.predict <- egss_predict(yt = yt1$Observed.y,
                                tt = tt1$Time.t,
                                parms = sk.egss.parms$mles)

sk.egss.predict.init <- sk.egss.predict |>
  left_join(snailkites.paynesp.init |> select(Time.t, 
                                              observation.date,
                                              abundance.monitored), 
            by = "Time.t") |>
  ungroup()

FigS2a <- sk.egss.predict.init |>
  pivot_longer(cols = !c(Time.t, cell,observation.date),
               names_to = "group",
               values_to = "Abundance") |>
  drop_na(Abundance) |>
  ggplot(aes(x = observation.date, y = Abundance, fill = group))+
    geom_line(aes(color = group), linetype = "dashed") +
    geom_point(aes(shape = group), color = "black")+
    labs(x = "Observation date",
       y = "Population abundance",
       tag = expression(bold("(a)")),
       fill = "",
       color = "",
       shape = "")+
    scale_color_manual(values = c("black", "blue", "red"),
                       labels = c("Standardized Monitored","eBird observations", "Predicted EGSS - REMLE"))+
    scale_fill_manual(values = c("black", "blue", "red"),
                       labels = c("Standardized Monitored","eBird observations", "Predicted EGSS - REMLE"))+
    scale_shape_manual(values = c(19, 21, 24),
                       labels = c("Standardized Monitored","eBird observations", "Predicted EGSS - REMLE"))+
    theme_classic()+
    theme(legend.position = c(0.2, 0.8))
```


How much is the prediction out of the estimation?
```{r}

lm(REMLE~abundance.monitored, data = sk.egss.predict.init)

lm_eqn <- function(df, x, y){
    m <- lm(y ~ x, df);
    eq <- substitute(italic(y) == a + b %.% italic(x)*","~~italic(r)^2~"="~r2, 
         list(a = format(unname(coef(m)[1]), digits = 2),
              b = format(unname(coef(m)[2]), digits = 2),
             r2 = format(summary(m)$r.squared, digits = 3)))
    as.character(as.expression(eq));
}

FigS2b <- ggplot(sk.egss.predict.init, aes(x=abundance.monitored, y=REMLE))+
  geom_point()+
  geom_abline(slope = 1)+
  geom_smooth(method = "lm", color = "red", se = F)+
  scale_y_continuous(limits = c(0,80))+
  labs(x = "Observed abundance in Standardized methods",
       y = "Predicted abundance by EGSS from eBird",
       tag = expression(bold("(b)")))+
  geom_text(x = 50, y = 18, 
            label = lm_eqn(df = sk.egss.predict.init,
                           x = sk.egss.predict.init$abundance.monitored, 
                           y = sk.egss.predict.init$REMLE), 
            parse = TRUE, color = "red")+
  coord_fixed()+
  theme_classic()

#and combine in the figure

FigS2 <- grid.arrange(FigS2a, FigS2b, ncol = 2, widths = c(1.5, 1))

ggsave("results/FigS2_Step1.pdf", 
       plot = FigS2, dpi = 300, width = 10, height = 3.5, units = "in")

#or ####

plot(x = snailkites.egss.predict$abundance.monitored,
     y = snailkites.egss.predict$REMLE,
     xlim=c(0,80), ylim=c(0,80), type = "p", col = "black", pch = 19,
     ylab = "0bserved in Standardized methods",
     xlab = "Predicted by REMLE - EGSS")
abline(coef = c(0,1))
abline(factor_underestimation, col = "darkred")

## rounded coefficients for better output
cf <- round(coef(factor_underestimation), 2) 

## sign check to avoid having plus followed by minus for negative coefficients
eq <- paste0("Observed = ", cf[1],
             ifelse(sign(cf[2])==1, "+", "-"), abs(cf[2]), "(Predicted)")

## printing of the equation
mtext(eq, 3, line=-2, col = "darkred")

```

With the estimated model parameters and the data up to the first 2 years (95 weeks with observation), the probability that the population will crash below a critical threshold ($N_{critical}$, as the critical number of individuals) is estimated for the next two years (104 weeks). Let assume this $N_{critical} = 1/2 (\exp\bar{(y)})$. The resulting probability is recorded.

```{r}
#variables:
yt = snailkites.PP |>
  ungroup() |>
  arrange(Time.t) |>
  drop_na(Observed.y) |>
  select(Observed.y) |>
  log()

yt <- yt$Observed.y

tt <- snailkites.PP |>
  ungroup() |>
  arrange(Time.t) |>
  drop_na(Observed.y) |>
  select(Time.t)

tt <- tt$Time.t

last.tt <- nrow(yt1) #the first two years length

#Define N_critical
N.critical = (1/2)*mean(exp(yt))

#number of simulations
ntraj = 1000

#threshold times
thres.times <- 0:104

#Maximum simulation length
len.sim <- max(thres.times)+1

#last observed time (week)
m = last(tt[1:last.tt])

#plot the abundance data
plot(tt[1:last.tt], 
     exp(yt[1:last.tt]), 
     type="b", lwd=2, cex.lab=1.25, col = "blue", pch = 1,
     xlim=c(min(tt),max(tt)),
     ylim=c(0,100), 
     ylab="Population abundance", xlab="Time (week since 2018-01-01)")
points(x = sk.egss.predict.init$Time.t,
     y = sk.egss.predict.init$REMLE,
     type = "b", col = "red", pch = 2)
points(x = sk.egss.predict.init$Time.t,
     y = sk.egss.predict.init$abundance.monitored,
     type = "p", col = "black", pch = 19)
legend("topright", legend=c("Observed eBird", 
                           "Predicted EGSS - REMLE", 
                           "Standardized Monitored",
                           "Projected above threshold",
                           "Projected below threshold"
                           ),
       col=c("blue","red", "black","#d3d3d398","#FF000098"
             ), 
       lty=c(1,2,NA,1,1
             ), pch = c(1,1,19,NA,NA
                        ), cex=0.8)

#initiating array to store last points of simulation at time m+5
last.points <- rep(0,ntraj)

#simulate population 100 trends 
for(j in 1:ntraj){
  sim.mat <- egss_sim(1,
                      tt = thres.times,
                      parms = sk.egss.parms$mles)
  Pop.sim <- exp(c(yt[last.tt], sim.mat[-1]))
  
  if(Pop.sim[len.sim]>=N.critical){
    lines(m+thres.times, Pop.sim, col="#d3d3d318", lty = "solid")
  }else{lines(m+thres.times, Pop.sim, col="#FF000018", lty = "solid")}
  
  last.points[j] <- Pop.sim[len.sim]
}

#add critical value line
abline(h=N.critical, lty=2, lwd=1)	

#How many of the simulations (ntraj) end below N_critical? Probability of extinction
P.ext1 <- length(which(round(last.points,2) <= N.critical))/ntraj

#Lognormal with library(ke31d)
kde.sims <- kde1d(x=last.points)

P.ext2 <- pkde1d(q=N.critical, obj=kde.sims)

phi <- 1-(mean(c(P.ext1, P.ext2)))

lines(x = (kde.sims$values*250 + m + last(thres.times)),
      y = kde.sims$grid_points,
      col="#d3d3d398", lwd=2, lty=1)

lines(x = (kde.sims$values[kde.sims$grid_points <= N.critical]*250 + m + last(thres.times)),
      y = kde.sims$grid_points[kde.sims$grid_points <= N.critical],
      col="#FF000098", lwd=2, lty=1)


title(main=paste0("EGSS - P(local persistence) = ", signif(phi, digits=2)), cex=1.5)

#Exported pdf 10x5
```

Now, we can add the next month of observations (week 108; ending January 2020) to the time-series, and the model parameters are re-estimated as well as the probability of crashing below the $N_{critical}$ for the next two years (104 weeks). For this week, we will try to fit a OUSS as well, and decide by the density dependence parameter


```{r}
last.tt <- which(tt==108)

yt[1:last.tt] #It was already converted to the log-abundance
tt[1:last.tt]

OUSS.partial <- ouss_remle(yt = yt[1:last.tt],
                           tt = tt[1:last.tt],
                           fguess = guess_ouss(yt = yt[1:last.tt],
                                               tt = tt[1:last.tt]))

OUSS.partial$remles 

model <- if(OUSS.partial$remles[2] < 0.025){"EGSS"}else{"OUSS"}

EGSS.partial <- egss_mle(yt = yt[1:last.tt], 
                         tt = tt[1:last.tt],
                         fguess_egss = guess_egss(yt = yt[1:last.tt],
                                                  tt = tt[1:last.tt]))
sk.egss.predict1 <- egss_predict(yt = yt[1:last.tt],
                                         tt = tt[1:last.tt],
                                         parms = EGSS.partial$mles)

#N.critical, len.sim, and ntraj were already correct
  #but m changes

m = last(tt[1:last.tt])

sk.egss.predict1 <- sk.egss.predict1 |>
  left_join(snailkites.PP |> select(Time.t, 
                                              observation.date,
                                              abundance.monitored), 
            by = "Time.t") |>
  ungroup()

#plot the abundance data
plot(tt[1:last.tt], 
     exp(yt[1:last.tt]), 
     type="b", lwd=2, cex.lab=1.25, col = "blue",
     xlim=c(min(tt),max(tt)),
     ylim=c(0,100), 
     ylab="Population abundance", xlab="Time (week since 2018-01-01)")
points(x = sk.egss.predict1$Time.t,
     y = sk.egss.predict1$REMLE,
     type = "b", col = "red", pch = 2)
points(x = sk.egss.predict1$Time.t,
     y = sk.egss.predict1$abundance.monitored,
     type = "p", col = "black", pch = 19)

last.points <- rep(0,ntraj)
#simulate population 100 trends 
for(j in 1:ntraj){
  sim.mat <- egss_sim(1,
                      tt = thres.times,
                      parms = EGSS.partial$mles)
  Pop.sim <- exp(c(yt[last.tt], sim.mat[-1]))
  
  if(Pop.sim[len.sim]>=N.critical){
    lines(m+thres.times, Pop.sim, col="#d3d3d318", lty = "solid")
  }else{lines(m+thres.times, Pop.sim, col="#FF000018", lty = "solid")}
  
  last.points[j] <- Pop.sim[len.sim]
}

#add critical value line
abline(h=N.critical, lty=2, lwd=1, col = "black")	

#How many of the simulations (ntraj) end below N_critical? Probability of extinction
P.ext1 <- length(which(round(last.points,2) <= N.critical))/ntraj

#Lognormal with library(ke31d)
kde.sims <- kde1d(x=last.points)

P.ext2 <- pkde1d(q=N.critical, obj=kde.sims)

phi <- 1-(mean(c(P.ext1, P.ext2)))

lines(x = (kde.sims$values*250 + m + last(thres.times)),
      y = kde.sims$grid_points,
      col="#d3d3d398", lwd=2, lty=1)

lines(x = (kde.sims$values[kde.sims$grid_points <= N.critical]*250 + m + last(thres.times)),
      y = kde.sims$grid_points[kde.sims$grid_points <= N.critical],
      col="#FF000098", lwd=2, lty=1)


title(main=paste0("EGSS - P(local persistence) = ", signif(phi, digits=2)), cex=1.5)

```


Iterating this process, for instance nine moments every 28 weeks (7 months):

```{r}

ntraj = 1000

m <- seq(104, 334, 28)

#A vector of time steps to evaluate (position)
last.tt <- rep(0,length(m))
for(i in 1:length(m)){
    last.tt[i] <- which(tt == m[i])
  }

#array to store p.below critical point (last point < N.critical/ntraj)
  phi <- rep(0,length(last.tt))
  
  model <- rep(0,length(last.tt))

  #Layout of 3 rows and 3 columnsx
  par(mfrow=c(3,3),mar=c(3,3,3,1),  oma=c(2,2,2,1), mgp=c(2,0.5,0))

  for(t in last.tt){
    plot(tt[1:t],
         exp(yt[1:t]),
         type="b", lwd=2, cex.lab=1.25, col = "blue",
         xlim=c(min(tt),max(tt)),
         ylim=c(0,90),
         ylab="Population abundance", xlab="Time (week since 2018-01-01)")
    
    last.points <- rep(0,ntraj)

    OUSS.partial <- ouss_remle(yt = yt[1:t],
                               tt = tt[1:t],
                               fguess = guess_ouss(yt = yt[1:t],
                                                   tt = tt[1:t]))

    model[t] <- if(OUSS.partial$remles[2] < 0.025){
      "EGSS"
        }else{
          "OUSS"
          }
  
    if(model == "OUSS"){
      parcial.predict <- ouss_predict(yt = yt[1:t],
                                      tt = tt[1:t],
                                      parms = OUSS.partial$remles,
                                      plot.it = F)
      
      parcial.predict <- as.data.frame(parcial.predict[1])
      
      points(x = parcial.predict$tt,
             y = parcial.predict$Predict.t,
             type = "b", col = "red", pch = 2)
      
      for(j in 1:ntraj){
        sim.mat <- ouss_sim(1,
                            tt = thres.times,
                            parms = OUSS.partial$remles)
      
      Pop.sim <- exp(c(yt[t], sim.mat[-1]))
  
        if(Pop.sim[len.sim]>=N.critical){
          lines(tt[t]+thres.times, Pop.sim, col="#d3d3d318", lty = "solid")
        }else{
          lines(tt[t]+thres.times, Pop.sim, col="#FF000018", lty = "solid")}
  
      last.points[j] <- Pop.sim[len.sim]
      }
      
      abline(h=N.critical, lty=2, lwd=1)	
      
      P.ext1 <- length(which(round(last.points,2) <= N.critical))/ntraj
      
      #Lognormal with library(ke31d)
      kde.sims <- kde1d(x=last.points)

      P.ext2 <- pkde1d(q=N.critical, obj=kde.sims)

      phi[t] <- 1-(mean(c(P.ext1, P.ext2)))

      lines(x = (kde.sims$values*250 + tt[t] + last(thres.times)),
            y = kde.sims$grid_points,
            col="#d3d3d398", lwd=2, lty=1)
      
      
      lines(x = (kde.sims$values[kde.sims$grid_points <= N.critical]*250 + tt[t] + last(thres.times)),
      y = kde.sims$grid_points[kde.sims$grid_points <= N.critical],
      col="#FF000098", lwd=2, lty=1)

      title(main=paste0(tt[t], " - ", model[t], " - P(local persistence) = ", signif(phi[t], digits=2)), cex=1.5)}
  
    else{
      
      EGSS.partial <- egss_mle(yt = yt[1:t],
                           tt = tt[1:t],
                           fguess_egss = guess_egss(yt = yt[1:t],
                                               tt = tt[1:t]))
    
      parcial.predict <- egss_predict(yt = yt[1:t],
                                      tt = tt[1:t],
                                      parms = EGSS.partial$mles)
      
      points(x = parcial.predict$Time.t,
             y = parcial.predict$REMLE,
             type = "b", col = "red", pch = 2)
      
    for(j in 1:ntraj){
    
      sim.mat <- egss_sim(1,
                          tt = thres.times,
                          parms = EGSS.partial$mles)
      Pop.sim <- exp(c(yt[t], sim.mat[-1]))
  
        if(Pop.sim[len.sim]>=N.critical){
          lines(tt[t]+thres.times, Pop.sim, col="#d3d3d318", lty = "solid")
        }else{
          lines(tt[t]+thres.times, Pop.sim, col="#FF000018", lty = "solid")}
  
      last.points[j] <- Pop.sim[len.sim]
      }
      
      abline(h=N.critical, lty=2, lwd=1)	
      
      P.ext1 <- length(which(round(last.points,2) <= N.critical))/ntraj
      
      #Lognormal with library(ke31d)
      kde.sims <- kde1d(x=last.points)

      P.ext2 <- pkde1d(q=N.critical, obj=kde.sims)

      phi[t] <- 1-(mean(c(P.ext1, P.ext2)))

      lines(x = (kde.sims$values*250 + tt[t] + last(thres.times)),
            y = kde.sims$grid_points,
            col="#d3d3d398", lwd=2, lty=1)
      
      
      lines(x = (kde.sims$values[kde.sims$grid_points <= N.critical]*250 + tt[t] + last(thres.times)),
      y = kde.sims$grid_points[kde.sims$grid_points <= N.critical],
      col="#FF000098", lwd=2, lty=1)

      title(main=paste0(tt[t], " - ", model[t], " - P(local persistence) = ", signif(phi[t], digits=2)), cex=1.5)}

}

#save pdf  
  
par(mfrow=c(1,1))

```

Let iterates for each time period

```{r}
ntraj = 1000

#A vector of the time step (week)
m <- seq(104, 334, 4)

#A vector of position for each time steps to evaluate within m
last.tt <- rep(0,length(m))
for(i in 1:length(m)){
    last.tt[i] <- which(tt == m[i])
  }

#array to store p.below critical point (last point < N.critical/ntraj)
  phi <- rep(0,length(last.tt))
  
  model <- rep(0,length(last.tt))

  #Layout of 3 rows and 3 columns
  par(mfrow=c(2,1),mar=c(3,3,3,1),  oma=c(2,2,2,1), mgp=c(2,0.5,0))

  for(t in last.tt){
    plot(tt[1:t],
         exp(yt[1:t]),
         type="b", lwd=2, cex.lab=1.25, col = "blue",
         xlim=c(min(tt),max(tt)),
         ylim=c(0,90),
         ylab="Population abundance", xlab="Time (week since 2018-01-01)")
    
    last.points <- rep(0,ntraj)

    OUSS.partial <- ouss_remle(yt = yt[1:t],
                               tt = tt[1:t],
                               fguess = guess_ouss(yt = yt[1:t],
                                                   tt = tt[1:t]))

    model[t] <- if(OUSS.partial$remles[2] < 0.025){
      "EGSS"
        }else{
          "OUSS"
          }
  
    if(model[t] == "OUSS"){
      parcial.predict <- ouss_predict(yt = yt[1:t],
                                      tt = tt[1:t],
                                      parms = OUSS.partial$remles,
                                      plot.it = F)
      
      parcial.predict <- as.data.frame(parcial.predict[1])
      
      points(x = parcial.predict$tt,
             y = parcial.predict$Predict.t,
             type = "b", col = "red", pch = 2)
      
      for(j in 1:ntraj){
        sim.mat <- ouss_sim(1,
                            tt = thres.times,
                            parms = OUSS.partial$remles)
      
      Pop.sim <- exp(c(yt[t], sim.mat[-1]))
  
        if(Pop.sim[len.sim]>=N.critical){
          lines(tt[t]+thres.times, Pop.sim, col="#d3d3d318", lty = "solid")
        }else{
          lines(tt[t]+thres.times, Pop.sim, col="#FF000018", lty = "solid")}
  
      last.points[j] <- Pop.sim[len.sim]
      }
      
      abline(h=N.critical, lty=2, lwd=1)	
      
      P.ext1 <- length(which(round(last.points,2) <= N.critical))/ntraj
      
      #Lognormal with library(ke31d)
      kde.sims <- kde1d(x=last.points)

      P.ext2 <- pkde1d(q=N.critical, obj=kde.sims)

      phi[t] <- 1-(mean(c(P.ext1, P.ext2)))

      lines(x = (kde.sims$values*250 + tt[t] + last(thres.times)),
            y = kde.sims$grid_points,
            col="#d3d3d398", lwd=2, lty=1)
      
      
      lines(x = (kde.sims$values[kde.sims$grid_points <= N.critical]*250 + tt[t] + last(thres.times)),
      y = kde.sims$grid_points[kde.sims$grid_points <= N.critical],
      col="#FF000098", lwd=2, lty=1)

      title(main=paste0(tt[t], " - ", model[t], " - P(local persistence) = ", signif(phi[t], digits=2)), cex=1.5)}
  
    else{
      
      EGSS.partial <- egss_mle(yt = yt[1:t],
                           tt = tt[1:t],
                           fguess_egss = guess_egss(yt = yt[1:t],
                                               tt = tt[1:t]))
    
      parcial.predict <- egss_predict(yt = yt[1:t],
                                      tt = tt[1:t],
                                      parms = EGSS.partial$mles)
      
      points(x = parcial.predict$Time.t,
             y = parcial.predict$REMLE,
             type = "b", col = "red", pch = 2)
      
    for(j in 1:ntraj){
    
      sim.mat <- egss_sim(1,
                          tt = thres.times,
                          parms = EGSS.partial$mles)
      Pop.sim <- exp(c(yt[t], sim.mat[-1]))
  
        if(Pop.sim[len.sim]>=N.critical){
          lines(tt[t]+thres.times, Pop.sim, col="#d3d3d318", lty = "solid")
        }else{
          lines(tt[t]+thres.times, Pop.sim, col="#FF000018", lty = "solid")}
  
      last.points[j] <- Pop.sim[len.sim]
      }
      
      abline(h=N.critical, lty=2, lwd=1)	
      
      P.ext1 <- length(which(round(last.points,2) <= N.critical))/ntraj
      
      #Lognormal with library(ke31d)
      kde.sims <- kde1d(x=last.points)

      P.ext2 <- pkde1d(q=N.critical, obj=kde.sims)

      phi[t] <- 1-(mean(c(P.ext1, P.ext2)))

      lines(x = (kde.sims$values*250 + tt[t] + last(thres.times)),
            y = kde.sims$grid_points,
            col="#d3d3d398", lwd=2, lty=1)
      
      
      lines(x = (kde.sims$values[kde.sims$grid_points <= N.critical]*250 + tt[t] + last(thres.times)),
      y = kde.sims$grid_points[kde.sims$grid_points <= N.critical],
      col="#FF000098", lwd=2, lty=1)

      title(main=paste0(tt[t], " - ", model[t], " - P(local persistence) = ", signif(phi[t], digits=2)), cex=1.5)}

  }
  
  par(mfrow=c(1,1))
```

Recovering the data

```{r}
localpersist <- data.frame(phi = phi,
                           model = model,
                           t = c(1:length(phi)))

localpersist$Time.t <- tt[localpersist$t]

inset.plot <- ggplot(snailkites.PP, aes(x = observation.date, 
                                       y = abundance.monitored))+
              geom_point(color = "black")+
              labs(x = "",
                   y = "")+
                theme_classic()+
              theme(panel.border = element_rect(colour = "black", fill=NA, size=1))
  

coeff <- 100

main.plot <- snailkites.PP |>
  left_join(localpersist) |> 
  ungroup() |>
  mutate(phi = phi*coeff) |>
  pivot_longer(cols = !c(Time.t, cell, year, week, observation.date, model,t),
               names_to = "group",
               values_to = "Abundance") |>
  drop_na(Abundance) |> filter(Abundance > 0) |>
  ggplot(aes(x = observation.date, y = Abundance, fill = group))+
    geom_vline(xintercept = snailkites.PP$observation.date[95], color = "gray", linetype = "dotted")+
    geom_line(aes(color = group), linetype = "solid", alpha = 0.5) +
    geom_point(aes(shape = group), color = "black", alpha = 0.75)+
    labs(x = "Observation date",
       y = "Population abundance",
       tag = "",
       fill = "",
       color = "",
       shape = "") +
  scale_y_continuous(limits = c(0,100),
                     sec.axis = sec_axis(trans = ~./100, 
                                         name = "Probability (Local persistence)"))+
    scale_color_manual(values = c("white", "blue", "red"),
                       labels = c("Standardized Monitored","eBird observations", "P(Local persistence)"))+
    scale_fill_manual(values = c("black", "blue", "red"),
                       labels = c("Standardized Monitored","eBird observations", "P(Local persistence)"))+
    scale_shape_manual(values = c(19, 21,23),
                       labels = c("Standardized Monitored","eBird observations", "P(Local persistence)"))+
    theme_classic()+
    theme(legend.position = "bottom")+
    theme(axis.line.y.right = element_line(color = "red"), 
       axis.ticks.y.right = element_line(color = "red"))

library(cowplot)

plot.with.inset <-
  ggdraw() +
  draw_plot(main.plot) +
  draw_plot(inset.plot, x = 0.053, y = .7, width = .27, height = .25)

```
